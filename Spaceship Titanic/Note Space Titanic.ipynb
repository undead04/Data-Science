{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8277eab9-654d-407d-850e-fa2a9c9282a8",
   "metadata": {},
   "source": [
    "## Ghi ch√∫:\n",
    "- Nh·ªØng b∆∞·ªõc c·∫ßn l√†m khi l√†m m·ªôt m√¥ h√¨nh m√°y h·ªçc:\n",
    "  * B∆∞·ªõc 1: X√°c ƒë·ªãnh v·∫•n ƒë·ªÅ\n",
    "  * B∆∞·ªõc 2: Thu nh·∫≠p d·ªØ li·ªáu\n",
    "  * B∆∞·ªõc 3: Chu·∫©n b·ªã s·ªØ l√≠ d·ªØ li·ªáu\n",
    "  * B∆∞·ªõc 4: Ph√¢n tich d·ªØ li·ªáu thƒÉm d√≤ b·∫±ng th·ªëng k√™\n",
    "  * B∆∞·ªõc 5: X√¢y d·ª±ng m√¥ h√¨nh d·ªØ li·ªáu\n",
    "  * B∆∞·ªõc 6: ƒê√°nh gi√° m√¥ h√¨nh\n",
    "  * B∆∞·ªõc 7: Tinh ch·ªânh m√¥ h√¨nh b·∫±ng si√™u tham s·ªë\n",
    "  * B∆∞·ªõc 8: Tinh ch·ªânh m√¥ h√¨nh b·∫±ng ch·ªçn l·ªçc ƒë·∫∑c tr∆∞ng\n",
    "  * B∆∞·ªõc 9: X√°c th·ª±c v√† tri·ªÉn khai\n",
    "  * B∆∞·ªõc 10: K·∫øt lu·∫≠n t·ªëi ∆∞u h√≥a v√† chi·∫øn l∆∞·ª£c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f337e1-5cb0-46cd-9a05-016d91cd619d",
   "metadata": {},
   "source": [
    "# B∆∞·ªõc 3: Chu·∫©n b·ªã s·ªØ l√≠ d·ªØ li·ªáu\n",
    "## B∆∞·ªõc 3.1: Import th∆∞ vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7d6c68-d9fa-475f-af8e-c55eff6e5a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load Th∆∞ vi·ªán\n",
    "import sys  # Ki·ªÉm tra phi√™n b·∫£n Python\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "\n",
    "import pandas as pd # X·ª≠ l√Ω d·ªØ li·ªáu d·∫°ng b·∫£ng (DataFrame).\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "import matplotlib # V·∫Ω bi·ªÉu ƒë·ªì khoa h·ªçc\n",
    "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
    "\n",
    "import numpy as np # T√≠nh to√°n khoa h·ªçc, x·ª≠ l√Ω m·∫£ng s·ªë h·ªçc\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "\n",
    "import scipy as sp # C√¥ng c·ª• to√°n h·ªçc n√¢ng cao\n",
    "print(\"SciPy version: {}\". format(sp.__version__)) \n",
    "\n",
    "import IPython\n",
    "from IPython import display # Hi·ªÉn th·ªã d·ªØ li·ªáu ƒë·∫πp h∆°n trong Jupyter Notebook\n",
    "print(\"IPython version: {}\". format(IPython.__version__)) \n",
    "\n",
    "import sklearn # C√°c thu·∫≠t to√°n Machine Learning\n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
    "\n",
    "import random # L√†m vi·ªác v·ªõi c√°c s·ªë ng·∫´u nhi√™n\n",
    "import time # X·ª≠ l√Ω th·ªùi gian\n",
    "\n",
    "\n",
    "# T·∫Øt c·∫£nh b√°o kh√¥ng quan tr·ªçng\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('-'*25)\n",
    "\n",
    "\n",
    "\n",
    "# Ch∆∞∆°ng tr√¨nh ki·ªÉm tra danh s√°ch t·ªáp trong th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu b·∫±ng l·ªánh:\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "719e4422-d46b-4bd8-b95c-949b8f07663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4dd21-df33-4136-a589-a91eb5fc3999",
   "metadata": {},
   "source": [
    "### C√°c thu·∫≠t to√°n ph·ªï bi·∫øn ƒë∆∞·ª£c s·ª≠ d·ª•ng:\n",
    "- ‚úÖ `svm` ‚Üí Support Vector Machines (SVM).\n",
    "- ‚úÖ `tree` ‚Üí Decision Tree Classifier.\n",
    "- ‚úÖ `linear_model` ‚Üí C√°c m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh (Logistic Regression).\n",
    "- ‚úÖ `neighbors` ‚Üí K-Nearest Neighbors (KNN).\n",
    "- ‚úÖ `naive_bayes` ‚Üí Naive Bayes Classifier.\n",
    "- ‚úÖ `ensemble` ‚Üí C√°c m√¥ h√¨nh ensemble nh∆∞ Random Forest, Gradient Boosting.\n",
    "- ‚úÖ `discriminant_analysis` ‚Üí Linear Discriminant Analysis (LDA).\n",
    "- ‚úÖ `gaussian_process` ‚Üí Gaussian Process Classifier.\n",
    "- ‚úÖ `XGBClassifier` t·ª´ xgboost ‚Üí M·ªôt m√¥ h√¨nh boosting m·∫°nh m·∫Ω."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2dbd5da-864f-49c7-ba35-2f131b8b7293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection, model_selection, metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d012956-649b-4ce3-ab3d-7d6d8dd1ba24",
   "metadata": {},
   "source": [
    "### C√°c c√¥ng c·ª• gi√∫p x·ª≠ l√Ω d·ªØ li·ªáu ƒë·∫ßu v√†o:\n",
    "- ‚úÖ `OneHotEncoder` & `LabelEncoder` ‚Üí M√£ h√≥a bi·∫øn ph√¢n lo·∫°i (categorical variables).-\n",
    "- ‚úÖ `feature_selection` ‚Üí Ch·ªçn c√°c ƒë·∫∑c tr∆∞ng quan tr·ªçng nh·∫•t.\n",
    "- ‚úÖ `model_selection` ‚Üí Chia t·∫≠p d·ªØ li·ªáu (train/test split), k-fold cross-validation.\n",
    "- ‚úÖ `metrics` ‚Üí C√°c ph√©p ƒë√°nh gi√° m√¥ h√¨nh (accuracy, F1-score, confusion matrix).\n",
    "- ‚úÖ `Pipeline` ‚Üí T·∫°o pipeline gi√∫p chu·∫©n h√≥a v√† hu·∫•n luy·ªán m√¥ h√¨nh d·ªÖ d√†ng h∆°n.\n",
    "- ‚úÖ `ColumnTransformer` ‚Üí D√πng ƒë·ªÉ x·ª≠ l√Ω c√°c c·ªôt d·ªØ li·ªáu kh√°c nhau theo c√°ch kh√°c nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167546b-709d-48b4-8109-9ccd3d3c70c7",
   "metadata": {},
   "source": [
    "## 3.2 Kh√°m ph√° d·ªØ li·ªáu (Meet and Greet Data)\n",
    "### M·ª•c ti√™u:\n",
    "- Ki·ªÉm tra s∆° b·ªô d·ªØ li·ªáu b·∫±ng c√°ch import v√† quan s√°t.\n",
    "- Nh·∫≠n di·ªán c√°c bi·∫øn ƒë·∫ßu v√†o (feature variables) v√† bi·∫øn m·ª•c ti√™u (target variable).\n",
    "- X√°c ƒë·ªãnh c√°c ki·ªÉu d·ªØ li·ªáu (s·ªë, chu·ªói, boolean).\n",
    "- Ki·ªÉm tra th√¥ng tin thi·∫øu d·ªØ li·ªáu\n",
    "### V√≠ d·ª•:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69b2de85-087a-44a4-a3dd-6e18434dab2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8693 entries, 0 to 8692\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   PassengerId   8693 non-null   object \n",
      " 1   HomePlanet    8492 non-null   object \n",
      " 2   CryoSleep     8476 non-null   object \n",
      " 3   Cabin         8494 non-null   object \n",
      " 4   Destination   8511 non-null   object \n",
      " 5   Age           8514 non-null   float64\n",
      " 6   VIP           8490 non-null   object \n",
      " 7   RoomService   8512 non-null   float64\n",
      " 8   FoodCourt     8510 non-null   float64\n",
      " 9   ShoppingMall  8485 non-null   float64\n",
      " 10  Spa           8510 non-null   float64\n",
      " 11  VRDeck        8505 non-null   float64\n",
      " 12  Name          8493 non-null   object \n",
      " 13  Transported   8693 non-null   bool   \n",
      "dtypes: bool(1), float64(6), object(7)\n",
      "memory usage: 891.5+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2192</th>\n",
       "      <td>2347_04</td>\n",
       "      <td>Earth</td>\n",
       "      <td>True</td>\n",
       "      <td>G/377/P</td>\n",
       "      <td>PSO J318.5-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Joanry Wellierras</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>1474_01</td>\n",
       "      <td>Mars</td>\n",
       "      <td>True</td>\n",
       "      <td>F/287/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>32.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Morms Melte</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6208</th>\n",
       "      <td>6563_02</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>G/1067/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>30.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>784.0</td>\n",
       "      <td>Juanna Gainney</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4253</th>\n",
       "      <td>4524_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/930/P</td>\n",
       "      <td>PSO J318.5-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Briane Fulloydez</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId HomePlanet CryoSleep     Cabin    Destination   Age    VIP  \\\n",
       "2192     2347_04      Earth      True   G/377/P  PSO J318.5-22   0.0  False   \n",
       "1400     1474_01       Mars      True   F/287/P    TRAPPIST-1e  32.0  False   \n",
       "6208     6563_02      Earth     False  G/1067/S    TRAPPIST-1e  30.0  False   \n",
       "4253     4524_01      Earth     False   F/930/P  PSO J318.5-22   NaN    NaN   \n",
       "\n",
       "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck               Name  \\\n",
       "2192          0.0        0.0           0.0  0.0     0.0  Joanry Wellierras   \n",
       "1400          0.0        0.0           0.0  NaN     0.0        Morms Melte   \n",
       "6208          0.0      123.0           0.0  1.0   784.0     Juanna Gainney   \n",
       "4253          0.0      116.0        2048.0  2.0     0.0   Briane Fulloydez   \n",
       "\n",
       "      Transported  \n",
       "2192         True  \n",
       "1400         True  \n",
       "6208        False  \n",
       "4253         True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ƒê·ªçc file d·ªØ li·ªáu t·ª´ Kaggle\n",
    "data_raw = pd.read_csv('./data/train.csv')\n",
    "data_val = pd.read_csv('./data/test.csv')\n",
    "\n",
    "# Sao ch√©p d·ªØ li·ªáu ƒë·ªÉ l√†m s·∫°ch m√† kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn b·∫£n g·ªëc\n",
    "data1 = data_raw.copy(deep=True)\n",
    "\n",
    "# Danh s√°ch ch·ª©a c·∫£ t·∫≠p train v√† test (ti·ªán x·ª≠ l√Ω d·ªØ li·ªáu)\n",
    "data_cleaner = [data1, data_val]\n",
    "\n",
    "# Hi·ªÉn th·ªã th√¥ng tin t·ªïng quan v·ªÅ d·ªØ li·ªáu\n",
    "print(data_raw.info()) \n",
    "\n",
    "# Hi·ªÉn th·ªã 10 d√≤ng ng·∫´u nhi√™n trong t·∫≠p d·ªØ li·ªáu\n",
    "data_raw.sample(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bbcfa2e-91a9-4f51-869d-c6c59611151f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates in train set: 0, (0.0%)\n",
      "\n",
      "Duplicates in test set: 0, (0.0%)\n"
     ]
    }
   ],
   "source": [
    "#  Ki·ªÉu tra tr√πng d·ªØ li·ªáu\n",
    "print(f'Duplicates in train set: {data_raw.duplicated().sum()}, ({np.round(100*data_raw.duplicated().sum()/len(data_raw),1)}%)')\n",
    "print('')\n",
    "print(f'Duplicates in test set: {data_val.duplicated().sum()}, ({np.round(100*data_val.duplicated().sum()/len(data_val),1)}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f755c5d-27e3-4666-b8a6-2ad1d9cf9039",
   "metadata": {},
   "source": [
    "#### 3.21 T√≥m t·∫Øt 4 b∆∞·ªõc l√†m s·∫°ch d·ªØ li·ªáu (4 C's of Data Cleaning)\n",
    "- **1.Correction (Ch·ªânh s·ª≠a d·ªØ li·ªáu l·ªói, ngo·∫°i l·ªá)**:\n",
    "    * X√°c ƒë·ªãnh v√† s·ª≠a c√°c gi√° tr·ªã b·∫•t th∆∞·ªùng (outlier).\n",
    "    * V√≠ d·ª•: N·∫øu c√≥ h√†nh kh√°ch c√≥ tu·ªïi 800 thay v√¨ 80, c·∫ßn s·ª≠a l·∫°i ho·∫∑c lo·∫°i b·ªè.\n",
    "    * Tr√°nh ch·ªânh s·ª≠a d·ªØ li·ªáu g·ªëc tr·ª´ khi c√≥ l√Ω do r√µ r√†ng.\n",
    "- **2.Completing (Ho√†n thi·ªán d·ªØ li·ªáu b·ªã thi·∫øu - x·ª≠ l√Ω missing values)**:\n",
    "    * M·ªôt s·ªë thu·∫≠t to√°n kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c gi√° tr·ªã r·ªóng (NaN), n√™n c·∫ßn ƒëi·ªÅn gi√° tr·ªã h·ª£p l√Ω.\n",
    "    * D·ªØ li·ªáu ƒë·ªãnh t√≠nh (categorical) ‚Üí Th∆∞·ªùng ƒëi·ªÅn b·∫±ng mode (gi√° tr·ªã xu·∫•t hi·ªán nhi·ªÅu nh·∫•t).\n",
    "    * D·ªØ li·ªáu ƒë·ªãnh l∆∞·ª£ng (numerical) ‚Üí ƒêi·ªÅn b·∫±ng mean (trung b√¨nh), median (trung v·ªã) ho·∫∑c trung b√¨nh + ƒë·ªô l·ªách chu·∫©n ng·∫´u nhi√™n.\n",
    "    * V√≠ d·ª•:\n",
    "    * C·ªôt Age ‚Üí ƒêi·ªÅn gi√° tr·ªã median (trung v·ªã) ƒë·ªÉ tr√°nh ·∫£nh h∆∞·ªüng b·ªüi outlier.\n",
    "    * C·ªôt HomePlanet ‚Üí ƒêi·ªÅn b·∫±ng mode (h√†nh tinh ph·ªï bi·∫øn nh·∫•t).\n",
    "- **3.Creating (T·∫°o ƒë·∫∑c tr∆∞ng m·ªõi - Feature Engineering)**:\n",
    "    * D√πng c√°c c·ªôt hi·ªán c√≥ ƒë·ªÉ t·∫°o ra ƒë·∫∑c tr∆∞ng m·ªõi gi√∫p c·∫£i thi·ªán m√¥ h√¨nh.\n",
    "    * V√≠ d·ª•:\n",
    "    * C·ªôt Cabin c√≥ d·∫°ng deck/num/side, c√≥ th·ªÉ t√°ch th√†nh 3 c·ªôt ri√™ng (Deck, Num, Side).\n",
    "    * X√°c ƒë·ªãnh xem h·ªç c·ªßa h√†nh kh√°ch c√≥ ·∫£nh h∆∞·ªüng ƒë·∫øn vi·ªác ƒë∆∞·ª£c ch·ªçn kh√¥ng.\n",
    "- **4.Converting (Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu v·ªÅ d·∫°ng ph√π h·ª£p)**:\n",
    "    * Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu v·ªÅ ƒë√∫ng ki·ªÉu:\n",
    "    * Categorical (chu·ªói vƒÉn b·∫£n) ‚Üí Chuy·ªÉn th√†nh dummies (one-hot encoding) ƒë·ªÉ d·ªÖ t√≠nh to√°n.\n",
    "    * Boolean (True/False) ‚Üí Chuy·ªÉn th√†nh 0/1.\n",
    "    * V√≠ d·ª•:\n",
    "    * C·ªôt CryoSleep (True/False) ‚Üí ƒê·ªïi th√†nh 0/1.\n",
    "    * C·ªôt HomePlanet ‚Üí √Ånh x·∫° c√°c h√†nh tinh th√†nh s·ªë ho·∫∑c d√πng One-Hot Encoding.\n",
    "- üí° M·ª•c ti√™u: L√†m s·∫°ch, chu·∫©n b·ªã d·ªØ li·ªáu t·ªët h∆°n ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh AI/ML. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe70c257-342c-4ad0-b667-d0f543ebb1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns with null values:\n",
      " PassengerId       0\n",
      "HomePlanet      201\n",
      "CryoSleep       217\n",
      "Cabin           199\n",
      "Destination     182\n",
      "Age             179\n",
      "VIP             203\n",
      "RoomService     181\n",
      "FoodCourt       183\n",
      "ShoppingMall    208\n",
      "Spa             183\n",
      "VRDeck          188\n",
      "Name            200\n",
      "Transported       0\n",
      "dtype: int64\n",
      "----------\n",
      "Test/Validation columns with null values:\n",
      " PassengerId       0\n",
      "HomePlanet       87\n",
      "CryoSleep        93\n",
      "Cabin           100\n",
      "Destination      92\n",
      "Age              91\n",
      "VIP              93\n",
      "RoomService      82\n",
      "FoodCourt       106\n",
      "ShoppingMall     98\n",
      "Spa             101\n",
      "VRDeck           80\n",
      "Name             94\n",
      "dtype: int64\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8693</td>\n",
       "      <td>8492</td>\n",
       "      <td>8476</td>\n",
       "      <td>8494</td>\n",
       "      <td>8511</td>\n",
       "      <td>8514.000000</td>\n",
       "      <td>8490</td>\n",
       "      <td>8512.000000</td>\n",
       "      <td>8510.000000</td>\n",
       "      <td>8485.000000</td>\n",
       "      <td>8510.000000</td>\n",
       "      <td>8505.000000</td>\n",
       "      <td>8493</td>\n",
       "      <td>8693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8693</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6560</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8473</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>0001_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>G/734/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gollux Reedall</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>4602</td>\n",
       "      <td>5439</td>\n",
       "      <td>8</td>\n",
       "      <td>5915</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>4378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.827930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>224.687617</td>\n",
       "      <td>458.077203</td>\n",
       "      <td>173.729169</td>\n",
       "      <td>311.138778</td>\n",
       "      <td>304.854791</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.489021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>666.717663</td>\n",
       "      <td>1611.489240</td>\n",
       "      <td>604.696458</td>\n",
       "      <td>1136.705535</td>\n",
       "      <td>1145.717189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14327.000000</td>\n",
       "      <td>29813.000000</td>\n",
       "      <td>23492.000000</td>\n",
       "      <td>22408.000000</td>\n",
       "      <td>24133.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId HomePlanet CryoSleep    Cabin  Destination          Age  \\\n",
       "count         8693       8492      8476     8494         8511  8514.000000   \n",
       "unique        8693          3         2     6560            3          NaN   \n",
       "top        0001_01      Earth     False  G/734/S  TRAPPIST-1e          NaN   \n",
       "freq             1       4602      5439        8         5915          NaN   \n",
       "mean           NaN        NaN       NaN      NaN          NaN    28.827930   \n",
       "std            NaN        NaN       NaN      NaN          NaN    14.489021   \n",
       "min            NaN        NaN       NaN      NaN          NaN     0.000000   \n",
       "25%            NaN        NaN       NaN      NaN          NaN    19.000000   \n",
       "50%            NaN        NaN       NaN      NaN          NaN    27.000000   \n",
       "75%            NaN        NaN       NaN      NaN          NaN    38.000000   \n",
       "max            NaN        NaN       NaN      NaN          NaN    79.000000   \n",
       "\n",
       "          VIP   RoomService     FoodCourt  ShoppingMall           Spa  \\\n",
       "count    8490   8512.000000   8510.000000   8485.000000   8510.000000   \n",
       "unique      2           NaN           NaN           NaN           NaN   \n",
       "top     False           NaN           NaN           NaN           NaN   \n",
       "freq     8291           NaN           NaN           NaN           NaN   \n",
       "mean      NaN    224.687617    458.077203    173.729169    311.138778   \n",
       "std       NaN    666.717663   1611.489240    604.696458   1136.705535   \n",
       "min       NaN      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       NaN      0.000000      0.000000      0.000000      0.000000   \n",
       "50%       NaN      0.000000      0.000000      0.000000      0.000000   \n",
       "75%       NaN     47.000000     76.000000     27.000000     59.000000   \n",
       "max       NaN  14327.000000  29813.000000  23492.000000  22408.000000   \n",
       "\n",
       "              VRDeck            Name Transported  \n",
       "count    8505.000000            8493        8693  \n",
       "unique           NaN            8473           2  \n",
       "top              NaN  Gollux Reedall        True  \n",
       "freq             NaN               2        4378  \n",
       "mean      304.854791             NaN         NaN  \n",
       "std      1145.717189             NaN         NaN  \n",
       "min         0.000000             NaN         NaN  \n",
       "25%         0.000000             NaN         NaN  \n",
       "50%         0.000000             NaN         NaN  \n",
       "75%        46.000000             NaN         NaN  \n",
       "max     24133.000000             NaN         NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X√°c ƒë·ªãnh c·ªôt n√†o c√≥ d·ªØ li·ªáu b·ªã thi·∫øu ƒë·ªÉ x·ª≠ l√Ω.\n",
    "print('Train columns with null values:\\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values:\\n', data_val.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "data_raw.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49a38494-b39a-402a-93b6-b6a61bf62b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId     8693\n",
       "HomePlanet         3\n",
       "CryoSleep          2\n",
       "Cabin           6560\n",
       "Destination        3\n",
       "Age               80\n",
       "VIP                2\n",
       "RoomService     1273\n",
       "FoodCourt       1507\n",
       "ShoppingMall    1115\n",
       "Spa             1327\n",
       "VRDeck          1306\n",
       "Name            8473\n",
       "Transported        2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc9674-47c8-439e-a2a9-f34cf8309edc",
   "metadata": {},
   "source": [
    "- Continuous Features (6 ƒë·∫∑c tr∆∞ng li√™n t·ª•c - s·ªë th·ª±c ho·∫∑c s·ªë nguy√™n)\n",
    "- Categorical Features (4 ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i - r·ªùi r·∫°c)\n",
    "- Descriptive/Qualitative Features (3 ƒë·∫∑c tr∆∞ng m√¥ t·∫£/ƒë·ªãnh t√≠nh - kh√¥ng tr·ª±c ti·∫øp d√πng ƒë·ªÉ d·ª± ƒëo√°n\n",
    "\n",
    "- M·ª•c ƒë√≠ch c·ªßa vi·ªác ph√¢n lo·∫°i n√†y:\n",
    "    * **C√°c ƒë·∫∑c tr∆∞ng li√™n t·ª•c** (Continuous) c√≥ th·ªÉ ƒë∆∞·ª£c chu·∫©n h√≥a ho·∫∑c x·ª≠ l√Ω outlier.\n",
    "    * **C√°c ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i** (Categorical) c·∫ßn ƒë∆∞·ª£c m√£ h√≥a (Encoding) ƒë·ªÉ ƒë∆∞a v√†o m√¥ h√¨nh.\n",
    "    * **C√°c ƒë·∫∑c tr∆∞ng m√¥ t·∫£** (Descriptive) c√≥ th·ªÉ ƒë∆∞·ª£c lo·∫°i b·ªè ho·∫∑c t√°ch th√†nh th√¥ng tin h·ªØu √≠ch h∆°n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "728746d9-fc30-4617-96fc-4d56fec58cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expenditure features\n",
    "exp_feats=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "# Categorical features\n",
    "cat_feats=['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\n",
    "\n",
    "# Qualitative features\n",
    "qual_feats=['PassengerId', 'Cabin' ,'Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d6f47-93fe-4aa9-9e03-534f9c7bd62f",
   "metadata": {},
   "source": [
    "### 3.22 L√†m s·∫°ch d·ªØ li·ªáu\n",
    "** Developer Documentation: **\n",
    "* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n",
    "* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n",
    "* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n",
    "* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-docs/stable/indexing.html)\n",
    "* [pandas.isnull](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html)\n",
    "* [pandas.DataFrame.sum](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sum.html)\n",
    "* [pandas.DataFrame.mode](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mode.html)\n",
    "* [pandas.DataFrame.copy](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.copy.html)\n",
    "* [pandas.DataFrame.fillna](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html)\n",
    "* [pandas.DataFrame.drop](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html)\n",
    "* [pandas.Series.value_counts](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html)\n",
    "* [pandas.DataFrame.loc](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd796fa-0c9a-48a7-99ca-57ec0bd4b791",
   "metadata": {},
   "source": [
    "##### Ghi ch√∫:\n",
    "- sns: countPlot,histPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b81661-ce55-45f7-9129-728ccc68155e",
   "metadata": {},
   "source": [
    "#### 3.23 M√£ h√≥a d·ªØ li·ªáu\n",
    "- Ch√∫ng t√¥i s·∫Ω chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu ph√¢n lo·∫°i th√†nh c√°c bi·∫øn gi·∫£ ƒë·ªÉ ph√¢n t√≠ch to√°n h·ªçc. C√≥ nhi·ªÅu c√°ch ƒë·ªÉ m√£ h√≥a c√°c bi·∫øn ph√¢n lo·∫°i; ch√∫ng t√¥i s·∫Ω s·ª≠ d·ª•ng c√°c h√†m sklearn v√† pandas\n",
    "-  **Developer Documentation:**\n",
    "* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n",
    "* [Sklearn LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "* [Sklearn OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "* [Pandas Categorical dtype](https://pandas.pydata.org/pandas-docs/stable/categorical.html)\n",
    "* [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b0cd7e-6f63-4f25-96b9-25ecd588c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n",
    "\n",
    "#code categorical data\n",
    "label = LabelEncoder()\n",
    "\n",
    "for data in data_cleaner:\n",
    "\n",
    "    data['HomePlanet_Code'] = label.fit_transform(data['HomePlanet'])\n",
    "    data['CryoSleep_Code'] = label.fit_transform(data['CryoSleep'])\n",
    "    data['Destination_Code'] = label.fit_transform(data['Destination'])\n",
    "    data['VIP_Code'] = label.fit_transform(data['VIP'])\n",
    "    data['Age_group_Code'] = label.fit_transform(data['Age_group'])\n",
    "    data['Cabin_deck_Code'] = label.fit_transform(data['Cabin_deck'])\n",
    "    data['Cabin_side_Code'] = label.fit_transform(data['Cabin_side'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d968e92-7653-4636-ad9d-c8177ddae90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define y variable aka target/outcome\n",
    "Target = ['Transported']\n",
    "\n",
    "# define x variables for original features aka feature selection\n",
    "\n",
    "# 'HomePlanet', 'CryoSleep', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'\n",
    "data1_x = ['HomePlanet', 'CryoSleep', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'] # Original data\n",
    "# 'HomePlanet', 'CryoSleep', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Transported', 'Age_group', 'Expenditure', 'No_spending', 'Group', 'Group_size', 'Solo', 'Cabin_deck', 'Cabin_number', 'Cabin_side', 'Cabin_region1', 'Cabin_region2', 'Cabin_region3', 'Cabin_region4', 'Cabin_region5', 'Cabin_region6', 'Cabin_region7', 'Family_size', 'HomePlanet_Code', 'CryoSleep_Code', 'Destination_Code', 'VIP_Code', 'Age_group_Code', 'Cabin_deck_Code', 'Cabin_side_Code'\n",
    "\n",
    "data1_x_calc = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Expenditure', 'No_spending', 'Group', 'Group_size', 'Solo', 'Cabin_number', 'Cabin_region1', 'Cabin_region2', 'Cabin_region3', 'Cabin_region4', 'Cabin_region5', 'Cabin_region6', 'Cabin_region7', 'Family_size', 'HomePlanet_Code', 'CryoSleep_Code', 'Destination_Code', 'VIP_Code', 'Age_group_Code', 'Cabin_deck_Code', 'Cabin_side_Code'] # coded for algorithm calculation\n",
    "data1_xy =  Target + data1_x\n",
    "print('Original X Y: ', data1_xy, '\\n')\n",
    "\n",
    "\n",
    "# define x variables for original w/bin features to remove continuous variables\n",
    "data1_x_bin = ['Age', 'No_spending', 'Group_size', 'Solo', 'Cabin_region1', 'Cabin_region2', 'Cabin_region3', 'Cabin_region4', 'Cabin_region5', 'Cabin_region6', 'Cabin_region7', 'Family_size', 'HomePlanet_Code', 'CryoSleep_Code', 'Destination_Code', 'VIP_Code', 'Age_group_Code', 'Cabin_deck_Code', 'Cabin_side_Code']\n",
    "data1_xy_bin = Target + data1_x_bin\n",
    "print('Bin X Y: ', data1_xy_bin, '\\n')\n",
    "\n",
    "\n",
    "#define x and y variables for dummy features original\n",
    "data1_dummy = pd.get_dummies(data1[data1_x])\n",
    "data1_x_dummy = data1_dummy.columns.tolist()\n",
    "data1_xy_dummy = Target + data1_x_dummy\n",
    "print('Dummy X Y: ', data1_xy_dummy, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "data1_dummy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e0016-e476-4d2a-9fee-c4173585d899",
   "metadata": {},
   "source": [
    "## 3.24 Ki·ªÉm tra d·ªØ li·ªáu ƒë√£ l√†m s·∫°ch Da-Double\n",
    "\n",
    "B√¢y gi·ªù ch√∫ng ta ƒë√£ l√†m s·∫°ch d·ªØ li·ªáu, h√£y th·ª±c hi·ªán ki·ªÉm tra da-double!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bbe83-97dd-4149-bbdb-ca46e2702bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train columns with null values: \\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (data1.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values: \\n', data_val.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (data_val.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "data_raw.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0019fbf9-9aea-4538-af75-2cd2fd17da2d",
   "metadata": {},
   "source": [
    "## 3.25 Ph√¢n chia d·ªØ li·ªáu ƒë√†o t·∫°o v√† th·ª≠ nghi·ªám\n",
    "\n",
    "Nh∆∞ ƒë√£ ƒë·ªÅ c·∫≠p tr∆∞·ªõc ƒë√≥, t·ªáp th·ª≠ nghi·ªám ƒë∆∞·ª£c cung c·∫•p th·ª±c s·ª± l√† d·ªØ li·ªáu x√°c th·ª±c ƒë·ªÉ g·ª≠i b√†i d·ª± thi. V√¨ v·∫≠y, ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng h√†m *sklearn* ƒë·ªÉ ph√¢n chia d·ªØ li·ªáu ƒë√†o t·∫°o th√†nh hai t·∫≠p d·ªØ li·ªáu; chia 75/25. ƒêi·ªÅu n√†y r·∫•t quan tr·ªçng, v√¨ v·∫≠y ch√∫ng ta kh√¥ng [qu√° ph√π h·ª£p v·ªõi m√¥ h√¨nh c·ªßa m√¨nh](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Nghƒ©a l√†, thu·∫≠t to√°n qu√° c·ª• th·ªÉ ƒë·ªëi v·ªõi m·ªôt t·∫≠p h·ª£p con nh·∫•t ƒë·ªãnh, n√™n n√≥ kh√¥ng th·ªÉ kh√°i qu√°t ch√≠nh x√°c m·ªôt t·∫≠p h·ª£p con kh√°c, t·ª´ c√πng m·ªôt t·∫≠p d·ªØ li·ªáu. ƒêi·ªÅu quan tr·ªçng l√† thu·∫≠t to√°n c·ªßa ch√∫ng ta ch∆∞a nh√¨n th·∫•y t·∫≠p h·ª£p con m√† ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng ƒë·ªÉ th·ª≠ nghi·ªám, v√¨ v·∫≠y n√≥ kh√¥ng \"gian l·∫≠n\" b·∫±ng c√°ch ghi nh·ªõ c√°c c√¢u tr·∫£ l·ªùi. Ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng h√†m train_test_split c·ªßa [*sklearn*](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). Trong c√°c ph·∫ßn sau, ch√∫ng ta c≈©ng s·∫Ω s·ª≠ d·ª•ng c√°c h√†m x√°c th·ª±c ch√©o c·ªßa [*sklearn*](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation), chia t·∫≠p d·ªØ li·ªáu c·ªßa ch√∫ng ta th√†nh t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm tra ƒë·ªÉ so s√°nh m√¥ h√¨nh d·ªØ li·ªáu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0883958-be5d-44b7-a194-39f3be3a7507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train and test data with function defaults\n",
    "#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n",
    "train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\n",
    "train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\n",
    "train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)\n",
    "\n",
    "\n",
    "print(\"Data1 Shape: {}\".format(data1.shape))\n",
    "print(\"Train1 Shape: {}\".format(train1_x.shape))\n",
    "print(\"Test1 Shape: {}\".format(test1_x.shape))\n",
    "\n",
    "train1_x_bin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15410809-8615-47a7-bb85-a8d0b118f699",
   "metadata": {},
   "source": [
    "<a id=\"ch6\"></a>\n",
    "# B∆∞·ªõc 4: Th·ª±c hi·ªán Ph√¢n t√≠ch thƒÉm d√≤ v·ªõi Th·ªëng k√™\n",
    "B√¢y gi·ªù d·ªØ li·ªáu c·ªßa ch√∫ng ta ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch, ch√∫ng ta s·∫Ω kh√°m ph√° d·ªØ li·ªáu c·ªßa m√¨nh b·∫±ng th·ªëng k√™ m√¥ t·∫£ v√† ƒë·ªì h·ªça ƒë·ªÉ m√¥ t·∫£ v√† t√≥m t·∫Øt c√°c bi·∫øn c·ªßa ch√∫ng ta. Trong giai ƒëo·∫°n n√†y, b·∫°n s·∫Ω th·∫•y m√¨nh ƒëang ph√¢n lo·∫°i c√°c t√≠nh nƒÉng v√† x√°c ƒë·ªãnh m·ªëi t∆∞∆°ng quan c·ªßa ch√∫ng v·ªõi bi·∫øn m·ª•c ti√™u v√† v·ªõi nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1405f7b6-5a26-47db-8608-882104ae2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discrete Variable Correlation by Survival using\n",
    "#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\n",
    "for x in data1_x:\n",
    "    if data1[x].dtype != 'float64' :\n",
    "        print('Transported Correlation by:', x)\n",
    "        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n",
    "        print('-'*10, '\\n')      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19568600-f43a-412f-8eb0-0806f5348698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation heatmap of dataset\n",
    "def correlation_heatmap(df):\n",
    "    _ , ax = plt.subplots(figsize =(14, 12))\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "    \n",
    "    _ = sns.heatmap(\n",
    "        df.corr(), \n",
    "        cmap = colormap,\n",
    "        square=True, \n",
    "        cbar_kws={'shrink':.9 }, \n",
    "        ax=ax,\n",
    "        annot=True, \n",
    "        linewidths=0.1,vmax=1.0, linecolor='white',\n",
    "        annot_kws={'fontsize': 5 }\n",
    "    )\n",
    "    \n",
    "    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "\n",
    "correlation_heatmap(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa6420c-be0b-44e5-8005-dc2695777c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pair plots of entire dataset\n",
    "pp = sns.pairplot(data1, hue = 'Transported', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\n",
    "pp.set(xticklabels=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35a1aab-44ae-4dca-8586-079aac64f088",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 5: M√¥ h√¨nh h√≥a d·ªØ li·ªáu  \n",
    "\n",
    "**Khoa h·ªçc d·ªØ li·ªáu** l√† m·ªôt lƒ©nh v·ª±c ƒëa ng√†nh k·∫øt h·ª£p gi·ªØa to√°n h·ªçc (th·ªëng k√™, ƒë·∫°i s·ªë tuy·∫øn t√≠nh, v.v.), khoa h·ªçc m√°y t√≠nh (ng√¥n ng·ªØ l·∫≠p tr√¨nh, h·ªá th·ªëng m√°y t√≠nh, v.v.) v√† qu·∫£n l√Ω kinh doanh (giao ti·∫øp, ki·∫øn th·ª©c chuy√™n m√¥n, v.v.). H·∫ßu h·∫øt c√°c nh√† khoa h·ªçc d·ªØ li·ªáu xu·∫•t th√¢n t·ª´ m·ªôt trong ba lƒ©nh v·ª±c n√†y, do ƒë√≥ h·ªç c√≥ xu h∆∞·ªõng nghi√™ng v·ªÅ lƒ©nh v·ª±c ƒë√≥. Tuy nhi√™n, khoa h·ªçc d·ªØ li·ªáu gi·ªëng nh∆∞ m·ªôt chi·∫øc gh·∫ø ba ch√¢n, kh√¥ng c√≥ ch√¢n n√†o quan tr·ªçng h∆°n ch√¢n n√†o. V√¨ v·∫≠y, b∆∞·ªõc n√†y s·∫Ω ƒë√≤i h·ªèi ki·∫øn th·ª©c n√¢ng cao v·ªÅ to√°n h·ªçc. Nh∆∞ng ƒë·ª´ng lo, ch√∫ng ta ch·ªâ c·∫ßn m·ªôt c√°i nh√¨n t·ªïng quan, v√† ch√∫ng ta s·∫Ω c√πng nhau kh√°m ph√°. Nh·ªù khoa h·ªçc m√°y t√≠nh, nhi·ªÅu c√¥ng vi·ªác ph·ª©c t·∫°p ƒë√£ ƒë∆∞·ª£c t·ª± ƒë·ªông h√≥a. Nh·ªØng b√†i to√°n t·ª´ng y√™u c·∫ßu b·∫±ng c·∫•p cao trong to√°n h·ªçc hay th·ªëng k√™, gi·ªù ƒë√¢y ch·ªâ c·∫ßn v√†i d√≤ng code. Cu·ªëi c√πng, ch√∫ng ta c·∫ßn hi·ªÉu bi·∫øt v·ªÅ kinh doanh ƒë·ªÉ t∆∞ duy gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ. Sau c√πng, gi·ªëng nh∆∞ vi·ªác hu·∫•n luy·ªán m·ªôt ch√∫ ch√≥ d·∫´n ƒë∆∞·ªùng, m√°y h·ªçc t·ª´ ch√∫ng ta ch·ª© kh√¥ng ph·∫£i ng∆∞·ª£c l·∫°i.  \n",
    "\n",
    "### **Gi·ªõi thi·ªáu v·ªÅ Machine Learning (ML)**  \n",
    "Machine Learning (ML) - H·ªçc m√°y - ƒë√∫ng nh∆∞ t√™n g·ªçi, l√† vi·ªác d·∫°y m√°y t√≠nh \"c√°ch suy nghƒ©\" thay v√¨ \"suy nghƒ© v·ªÅ c√°i g√¨\". M·∫∑c d√π lƒ©nh v·ª±c n√†y v√† d·ªØ li·ªáu l·ªõn (Big Data) ƒë√£ xu·∫•t hi·ªán t·ª´ l√¢u, nh∆∞ng g·∫ßn ƒë√¢y n√≥ ng√†y c√†ng ph·ªï bi·∫øn do r√†o c·∫£n ti·∫øp c·∫≠n th·∫•p h∆°n, cho c·∫£ doanh nghi·ªáp l·∫´n c√° nh√¢n. ƒêi·ªÅu n√†y v·ª´a l√† c∆° h·ªôi v·ª´a l√† th√°ch th·ª©c. ƒêi·ªÉm t√≠ch c·ª±c l√† ng√†y c√†ng c√≥ nhi·ªÅu ng∆∞·ªùi ti·∫øp c·∫≠n ƒë∆∞·ª£c c√°c thu·∫≠t to√°n m·∫°nh m·∫Ω ƒë·ªÉ gi·∫£i quy·∫øt nhi·ªÅu v·∫•n ƒë·ªÅ th·ª±c t·∫ø h∆°n. Tuy nhi√™n, ƒëi·ªÅu ti√™u c·ª±c l√† nhi·ªÅu ng∆∞·ªùi s·ª≠ d·ª•ng c√°c c√¥ng c·ª• n√†y m√† kh√¥ng th·ª±c s·ª± hi·ªÉu r√µ ch√∫ng, d·∫´n ƒë·∫øn nh·ªØng k·∫øt lu·∫≠n sai l·ªách.  \n",
    "\n",
    "Tr∆∞·ªõc ƒë√¢y, m√¨nh ƒë√£ t·ª´ng v√≠ von r·∫±ng n·∫øu b·∫°n y√™u c·∫ßu ai ƒë√≥ ƒë∆∞a cho b·∫°n m·ªôt tua v√≠t ƒë·∫ßu ch·ªØ th·∫≠p (*Philip screwdriver*), nh∆∞ng h·ªç l·∫°i ƒë∆∞a cho b·∫°n m·ªôt tua v√≠t d·∫πt ho·∫∑c t·ªá h∆°n l√† m·ªôt c√°i b√∫a, ƒëi·ªÅu ƒë√≥ s·∫Ω ph·∫£n √°nh r√µ s·ª± thi·∫øu hi·ªÉu bi·∫øt. Trong khoa h·ªçc d·ªØ li·ªáu, ƒëi·ªÅu n√†y c√≥ th·ªÉ khi·∫øn d·ª± √°n th·∫•t b·∫°i ho·∫∑c th·∫≠m ch√≠ ƒë∆∞a ra quy·∫øt ƒë·ªãnh sai l·∫ßm nghi√™m tr·ªçng. V√¨ v·∫≠y, thay v√¨ ch·ªâ h∆∞·ªõng d·∫´n b·∫°n **l√†m th·∫ø n√†o**, m√¨nh s·∫Ω gi·∫£i th√≠ch **t·∫°i sao b·∫°n l√†m nh∆∞ v·∫≠y**.  \n",
    "\n",
    "### **C√°c lo·∫°i Machine Learning**\n",
    "M·ª•c ƒë√≠ch c·ªßa ML l√† gi·∫£i quy·∫øt c√°c v·∫•n ƒë·ªÅ c·ªßa con ng∆∞·ªùi. ML c√≥ th·ªÉ ƒë∆∞·ª£c chia th√†nh ba lo·∫°i ch√≠nh:  \n",
    "\n",
    "1. **H·ªçc c√≥ gi√°m s√°t (*Supervised Learning*):** Cung c·∫•p cho m√¥ h√¨nh m·ªôt t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán ch·ª©a c√°c ƒë·∫ßu v√†o c√πng v·ªõi nh√£n ƒë·∫ßu ra ƒë√∫ng.  \n",
    "2. **H·ªçc kh√¥ng gi√°m s√°t (*Unsupervised Learning*):** D·ªØ li·ªáu hu·∫•n luy·ªán kh√¥ng c√≥ nh√£n, m√¥ h√¨nh t·ª± ph√°t hi·ªán ra c√°c m·∫´u t·ª´ d·ªØ li·ªáu.  \n",
    "3. **H·ªçc c·ªßng c·ªë (*Reinforcement Learning*):** M√¥ h√¨nh kh√¥ng ƒë∆∞·ª£c cung c·∫•p ngay ƒë√°p √°n ƒë√∫ng, m√† s·∫Ω nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi sau m·ªôt chu·ªói h√†nh ƒë·ªông ƒë·ªÉ d·∫ßn d·∫ßn h·ªçc h·ªèi.  \n",
    "\n",
    "Trong tr∆∞·ªùng h·ª£p c·ªßa ch√∫ng ta, b√†i to√°n y√™u c·∫ßu d·ª± ƒëo√°n m·ªôt h√†nh kh√°ch c√≥ \"s·ªëng s√≥t\" hay kh√¥ng, t·ª©c l√† m·ªôt b√†i to√°n **ph√¢n lo·∫°i c√≥ gi√°m s√°t (*Supervised Classification*)**.  \n",
    "\n",
    "### **C√°c thu·∫≠t to√°n Machine Learning ph·ªï bi·∫øn**\n",
    "C√≥ r·∫•t nhi·ªÅu thu·∫≠t to√°n ML, nh∆∞ng ch√∫ng c√≥ th·ªÉ ƒë∆∞·ª£c chia th√†nh b·ªën nh√≥m ch√≠nh:  \n",
    "- **Ph√¢n lo·∫°i (*Classification*)**: D√πng khi bi·∫øn m·ª•c ti√™u (*target variable*) c√≥ gi√° tr·ªã r·ªùi r·∫°c (nh∆∞ \"s·ªëng s√≥t\" ho·∫∑c \"kh√¥ng s·ªëng s√≥t\").  \n",
    "- **H·ªìi quy (*Regression*)**: D√πng khi bi·∫øn m·ª•c ti√™u c√≥ gi√° tr·ªã li√™n t·ª•c (nh∆∞ d·ª± ƒëo√°n gi√° nh√†).  \n",
    "- **Ph√¢n c·ª•m (*Clustering*)**: T√¨m c√°c nh√≥m d·ªØ li·ªáu t∆∞∆°ng t·ª± m√† kh√¥ng c·∫ßn nh√£n.  \n",
    "- **Gi·∫£m chi·ªÅu d·ªØ li·ªáu (*Dimensionality Reduction*)**: Gi·∫£m s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng ƒë·ªÉ m√¥ h√¨nh ƒë∆°n gi·∫£n h∆°n.  \n",
    "\n",
    "V√¨ b√†i to√°n c·ªßa ch√∫ng ta l√† **ph√¢n lo·∫°i (*Classification*)**, ta c√≥ th·ªÉ ch·ªçn m·ªôt trong c√°c thu·∫≠t to√°n d∆∞·ªõi ƒë√¢y:  \n",
    "- **Ensemble Methods** (H·ªçc t·∫≠p t·ªï h·ª£p)  \n",
    "- **Generalized Linear Models (GLM)** (H·ªìi quy tuy·∫øn t√≠nh, Logistic Regression)  \n",
    "- **Naive Bayes** (X√°c su·∫•t Bayes)  \n",
    "- **Nearest Neighbors** (*K-NN*)  \n",
    "- **Support Vector Machines (*SVM*)**  \n",
    "- **Decision Trees** (C√¢y quy·∫øt ƒë·ªãnh)  \n",
    "- **Discriminant Analysis** (Ph√¢n t√≠ch bi·ªát s·ªë)  \n",
    "\n",
    "### **L√†m th·∫ø n√†o ƒë·ªÉ ch·ªçn thu·∫≠t to√°n Machine Learning t·ªët nh·∫•t?**\n",
    "M·ªôt c√¢u h·ªèi ph·ªï bi·∫øn c·ªßa ng∆∞·ªùi m·ªõi h·ªçc ML l√† **\"Thu·∫≠t to√°n n√†o l√† t·ªët nh·∫•t?\"**. C√¢u tr·∫£ l·ªùi l√† **kh√¥ng c√≥ thu·∫≠t to√°n n√†o t·ªët nh·∫•t trong m·ªçi tr∆∞·ªùng h·ª£p**. ƒêi·ªÅu n√†y ƒë∆∞·ª£c ch·ª©ng minh qua **ƒê·ªãnh l√Ω Kh√¥ng b·ªØa tr∆∞a mi·ªÖn ph√≠ (*No Free Lunch Theorem - NFLT*)**, n√≥i r·∫±ng **kh√¥ng c√≥ thu·∫≠t to√°n n√†o v∆∞·ª£t tr·ªôi trong m·ªçi t√¨nh hu·ªëng**. Do ƒë√≥, c√°ch ti·∫øp c·∫≠n t·ªët nh·∫•t l√† th·ª≠ nghi·ªám nhi·ªÅu thu·∫≠t to√°n kh√°c nhau, tinh ch·ªânh ch√∫ng v√† so s√°nh k·∫øt qu·∫£.  \n",
    "\n",
    "M·ªôt s·ªë nghi√™n c·ª©u ƒë√£ so s√°nh c√°c thu·∫≠t to√°n ML, ch·∫≥ng h·∫°n nh∆∞:  \n",
    "- **Caruana & Niculescu-Mizil (2006):** So s√°nh c√°c thu·∫≠t to√°n ph·ªï bi·∫øn.  \n",
    "- **Ogutu et al. (2011):** ·ª®ng d·ª•ng trong ch·ªçn l·ªçc gen.  \n",
    "- **Fernandez-Delgado et al. (2014):** So s√°nh 179 b·ªô ph√¢n lo·∫°i t·ª´ 17 nh√≥m thu·∫≠t to√°n.  \n",
    "- **Thoma (2016):** So s√°nh thu·∫≠t to√°n trong *Scikit-learn*.  \n",
    "\n",
    "Ngo√†i ra, c√≥ m·ªôt quan ƒëi·ªÉm kh√°c: **\"D·ªØ li·ªáu nhi·ªÅu quan tr·ªçng h∆°n thu·∫≠t to√°n t·ªët\"**, nghƒ©a l√† m·ªôt thu·∫≠t to√°n ƒë∆°n gi·∫£n nh∆∞ng c√≥ nhi·ªÅu d·ªØ li·ªáu t·ªët v·∫´n c√≥ th·ªÉ cho k·∫øt qu·∫£ v∆∞·ª£t tr·ªôi h∆°n m·ªôt thu·∫≠t to√°n ph·ª©c t·∫°p nh∆∞ng d·ªØ li·ªáu ngh√®o n√†n.  \n",
    "\n",
    "### **Kh·ªüi ƒë·∫ßu v·ªõi Decision Tree, Random Forest, v√† Boosting**\n",
    "N·∫øu b·∫°n l√† ng∆∞·ªùi m·ªõi, m√¨nh khuy√™n b·∫°n n√™n b·∫Øt ƒë·∫ßu v·ªõi **C√¢y quy·∫øt ƒë·ªãnh (*Decision Tree*), R·ª´ng ng·∫´u nhi√™n (*Random Forest*) v√† Boosting**.  \n",
    "- Ch√∫ng d·ªÖ hi·ªÉu v√† d·ªÖ tri·ªÉn khai.  \n",
    "- Ch√∫ng d·ªÖ tinh ch·ªânh h∆°n so v·ªõi c√°c thu·∫≠t to√°n nh∆∞ *SVM*.  \n",
    "\n",
    "D∆∞·ªõi ƒë√¢y, m√¨nh s·∫Ω h∆∞·ªõng d·∫´n c√°ch ch·∫°y v√† so s√°nh nhi·ªÅu thu·∫≠t to√°n ML kh√°c nhau. Tuy nhi√™n, ph·∫ßn c√≤n l·∫°i c·ªßa b√†i n√†y s·∫Ω t·∫≠p trung v√†o vi·ªác h·ªçc c√°ch m√¥ h√¨nh h√≥a d·ªØ li·ªáu b·∫±ng **Decision Trees v√† c√°c bi·∫øn th·ªÉ c·ªßa n√≥**. üöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccee436-b9ba-4b84-8ba3-2ccca0f73cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning Algorithm (MLA) Selection and Initialization\n",
    "MLA = [\n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "\n",
    "    #Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model.RidgeClassifierCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    \n",
    "    #Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "    #SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "    \n",
    "    #Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "    XGBClassifier()    \n",
    "    ]\n",
    "\n",
    "#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "#note: this is an alternative to train_test_split\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "#create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "#create table to compare MLA predictions\n",
    "MLA_predict = data1[Target]\n",
    "\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "\n",
    "    #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "\n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
    "    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n",
    "    \n",
    "\n",
    "    #save MLA predictions - see section 6 for usage\n",
    "    alg.fit(data1[data1_x_bin], data1[Target])\n",
    "    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n",
    "    \n",
    "    row_index+=1\n",
    "    \n",
    "#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "MLA_compare\n",
    "#MLA_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4a5d0-a631-4acf-bb7c-0f8089b264af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\n",
    "sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n",
    "\n",
    "#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\n",
    "plt.title('Machine Learning Algorithm Accuracy Score \\n')\n",
    "plt.xlabel('Accuracy Score (%)')\n",
    "plt.ylabel('Algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c811a97-dda0-4e0e-afa4-2f7fbfaa39fd",
   "metadata": {},
   "source": [
    "## 5.1 ƒê√°nh gi√° Hi·ªáu su·∫•t M√¥ h√¨nh  \n",
    "\n",
    "H√£y c√πng t·ªïng k·∫øt l·∫°i: v·ªõi m·ªôt s·ªë b∆∞·ªõc l√†m s·∫°ch d·ªØ li·ªáu c∆° b·∫£n, ph√¢n t√≠ch d·ªØ li·ªáu v√† √°p d·ª•ng thu·∫≠t to√°n Machine Learning (MLA), ch√∫ng ta c√≥ th·ªÉ d·ª± ƒëo√°n vi·ªác v·∫≠n chuy·ªÉn h√†nh kh√°ch v·ªõi **ƒë·ªô ch√≠nh x√°c kho·∫£ng 80%**. Kh√¥ng t·ªá ch·ªâ v·ªõi v√†i d√≤ng code.  \n",
    "\n",
    "Nh∆∞ng c√¢u h·ªèi lu√¥n ƒë∆∞·ª£c ƒë·∫∑t ra l√†: **Ch√∫ng ta c√≥ th·ªÉ l√†m t·ªët h∆°n kh√¥ng?** V√† quan tr·ªçng h∆°n, **vi·ªác ƒë·∫ßu t∆∞ th·ªùi gian c√≥ x·ª©ng ƒë√°ng kh√¥ng?** V√≠ d·ª•, n·∫øu ch√∫ng ta ch·ªâ tƒÉng ƒë·ªô ch√≠nh x√°c th√™m **0.1%**, nh∆∞ng ph·∫£i m·∫•t **3 th√°ng ph√°t tri·ªÉn**, th√¨ c√≥ th·ª±c s·ª± ƒë√°ng kh√¥ng? N·∫øu b·∫°n l√†m trong lƒ©nh v·ª±c nghi√™n c·ª©u, c√¢u tr·∫£ l·ªùi c√≥ th·ªÉ l√† \"c√≥\". Nh∆∞ng n·∫øu b·∫°n l√†m trong kinh doanh, ph·∫ßn l·ªõn c√¢u tr·∫£ l·ªùi s·∫Ω l√† \"kh√¥ng\". V√¨ v·∫≠y, h√£y lu√¥n c√¢n nh·∫Øc ƒëi·ªÅu n√†y khi c·∫£i thi·ªán m√¥ h√¨nh c·ªßa m√¨nh.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Khoa h·ªçc D·ªØ li·ªáu 101: X√°c ƒë·ªãnh ƒê·ªô ch√≠nh x√°c C∆° b·∫£n (Baseline Accuracy)**  \n",
    "\n",
    "Tr∆∞·ªõc khi quy·∫øt ƒë·ªãnh **c·∫£i thi·ªán m√¥ h√¨nh**, ch√∫ng ta c·∫ßn x√°c ƒë·ªãnh xem m√¥ h√¨nh hi·ªán t·∫°i **c√≥ ƒë√°ng ƒë·ªÉ gi·ªØ l·∫°i kh√¥ng**. ƒê·ªÉ l√†m ƒëi·ªÅu ƒë√≥, h√£y quay v·ªÅ **ki·∫øn th·ª©c c∆° b·∫£n c·ªßa khoa h·ªçc d·ªØ li·ªáu**.  \n",
    "\n",
    "üü¢ **ƒê√¢y l√† m·ªôt b√†i to√°n nh·ªã ph√¢n**, v√¨ ch·ªâ c√≥ hai k·∫øt qu·∫£ c√≥ th·ªÉ x·∫£y ra:  \n",
    "1Ô∏è‚É£ H√†nh kh√°ch ƒë∆∞·ª£c v·∫≠n chuy·ªÉn.  \n",
    "2Ô∏è‚É£ H√†nh kh√°ch kh√¥ng ƒë∆∞·ª£c v·∫≠n chuy·ªÉn.  \n",
    "\n",
    "H√£y nghƒ© v·ªÅ n√≥ nh∆∞ m·ªôt **v·∫•n ƒë·ªÅ tung ƒë·ªìng xu**. N·∫øu b·∫°n c√≥ m·ªôt ƒë·ªìng xu c√¥ng b·∫±ng v√† ƒëo√°n m·∫∑t **ng·ª≠a ho·∫∑c s·∫•p**, th√¨ **x√°c su·∫•t ƒëo√°n ƒë√∫ng l√† 50%**. V√¨ v·∫≠y, ch√∫ng ta ƒë·∫∑t **50% l√† m·ª©c hi·ªáu su·∫•t m√¥ h√¨nh th·∫•p nh·∫•t**, b·ªüi v√¨ n·∫øu m√¥ h√¨nh c·ªßa b·∫°n c√≤n k√©m h∆°n m·ª©c n√†y, th√¨ t√¥i c√≥ th·ªÉ ƒë∆°n gi·∫£n **tung ƒë·ªìng xu** thay v√¨ d√πng m√¥ h√¨nh c·ªßa b·∫°n.  \n",
    "\n",
    "üöÄ **Nh∆∞ng ch√∫ng ta c√≥ d·ªØ li·ªáu!**  \n",
    "- Trong t·∫≠p d·ªØ li·ªáu c·ªßa ch√∫ng ta, c√≥ **4.378 / 8.693 (~50.4%)** h√†nh kh√°ch ƒë∆∞·ª£c v·∫≠n chuy·ªÉn.  \n",
    "- N·∫øu ch√∫ng ta **lu√¥n d·ª± ƒëo√°n r·∫±ng 100% h√†nh kh√°ch s·∫Ω ƒë∆∞·ª£c v·∫≠n chuy·ªÉn**, ch√∫ng ta s·∫Ω ƒë√∫ng **50.3%** th·ªùi gian.  \n",
    "\n",
    "üìå V√¨ v·∫≠y, h√£y ƒë·∫∑t **51% l√† m·ª©c hi·ªáu su·∫•t m√¥ h√¨nh k√©m**, b·ªüi v√¨ n·∫øu m√¥ h√¨nh c·ªßa b·∫°n k√©m h∆°n m·ª©c n√†y, t√¥i c√≥ th·ªÉ **ƒë∆°n gi·∫£n d·ª± ƒëo√°n theo t·∫ßn su·∫•t ph·ªï bi·∫øn nh·∫•t** m√† v·∫´n c√≥ k·∫øt qu·∫£ t·ªët h∆°n.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Khoa h·ªçc D·ªØ li·ªáu 101: C√°ch T·∫°o M√¥ h√¨nh c·ªßa Ri√™ng B·∫°n**  \n",
    "\n",
    "M√¥ h√¨nh c·ªßa ch√∫ng ta ƒëang d·∫ßn ƒë∆∞·ª£c c·∫£i thi·ªán, nh∆∞ng li·ªáu ch√∫ng ta c√≥ th·ªÉ **l√†m t·ªët h∆°n kh√¥ng**? Li·ªáu c√≥ **d·∫•u hi·ªáu (signal)** n√†o trong d·ªØ li·ªáu kh√¥ng?  \n",
    "\n",
    "üîç ƒê·ªÉ minh h·ªça ƒëi·ªÅu n√†y, ch√∫ng ta s·∫Ω x√¢y d·ª±ng m·ªôt **m√¥ h√¨nh c√¢y quy·∫øt ƒë·ªãnh (Decision Tree)**, v√¨ ƒë√¢y l√† m√¥ h√¨nh **d·ªÖ h√¨nh dung nh·∫•t** v√† ch·ªâ y√™u c·∫ßu **c√°c ph√©p t√≠nh c·ªông v√† nh√¢n ƒë∆°n gi·∫£n**.  \n",
    "\n",
    "üìå **C√°ch x√¢y d·ª±ng c√¢y quy·∫øt ƒë·ªãnh:**  \n",
    "- Ch√∫ng ta c·∫ßn ƒë·∫∑t c√°c c√¢u h·ªèi ƒë·ªÉ **ph√¢n nh√≥m d·ªØ li·ªáu** sao cho nh√≥m \"ƒë∆∞·ª£c v·∫≠n chuy·ªÉn\" (1) v√† \"kh√¥ng ƒë∆∞·ª£c v·∫≠n chuy·ªÉn\" (0) tr·ªü n√™n ƒë·ªìng nh·∫•t.  \n",
    "- ƒê√¢y l√† **s·ª± k·∫øt h·ª£p gi·ªØa khoa h·ªçc v√† ngh·ªá thu·∫≠t**, gi·ªëng nh∆∞ tr√≤ ch∆°i \"21 c√¢u h·ªèi\".  \n",
    "- N·∫øu b·∫°n mu·ªën th·ª±c h√†nh, h√£y t·∫£i t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán, nh·∫≠p v√†o **Excel** v√† t·∫°o **Pivot Table** ƒë·ªÉ ph√¢n t√≠ch t·ª´ng y·∫øu t·ªë sau.  \n",
    "\n",
    "üéØ **M·ª•c ti√™u:** X√¢y d·ª±ng m·ªôt c√¢y quy·∫øt ƒë·ªãnh ƒë·ªÉ **t√°ch nh√≥m \"ƒë∆∞·ª£c v·∫≠n chuy·ªÉn\" v√† \"kh√¥ng ƒë∆∞·ª£c v·∫≠n chuy·ªÉn\"**.  \n",
    "\n",
    "üöÄ **C√¢u h·ªèi ph√¢n nh√≥m (Decision Rules):**  \n",
    "1Ô∏è‚É£ **Tr·∫ª em (0-18 tu·ªïi) c√≥ t·ª∑ l·ªá ƒë∆∞·ª£c v·∫≠n chuy·ªÉn cao h∆°n.**  \n",
    "2Ô∏è‚É£ **Nh·ªØng ng∆∞·ªùi ƒë∆∞·ª£c v·∫≠n chuy·ªÉn c√≥ xu h∆∞·ªõng chi ti√™u √≠t h∆°n.**  \n",
    "3Ô∏è‚É£ **Ng∆∞·ªùi t·ª´ Tr√°i ƒê·∫•t c√≥ √≠t c∆° h·ªôi ƒë∆∞·ª£c v·∫≠n chuy·ªÉn h∆°n, trong khi ng∆∞·ªùi t·ª´ ch√¢u √Çu th√¨ may m·∫Øn h∆°n.**  \n",
    "4Ô∏è‚É£ **Nh·ªØng ng∆∞·ªùi ·ªü trong bu·ªìng ƒë√¥ng l·∫°nh (cryopod) c√≥ kh·∫£ nƒÉng ƒë∆∞·ª£c v·∫≠n chuy·ªÉn cao.**  \n",
    "5Ô∏è‚É£ **V·ªã tr√≠ c·ªßa cabin c≈©ng ·∫£nh h∆∞·ªüng ƒë·∫øn kh·∫£ nƒÉng v·∫≠n chuy·ªÉn.**  \n",
    "\n",
    "üìå **K·∫øt qu·∫£:**  \n",
    "- Ch·ªâ v·ªõi **m·ªôt s·ªë quy t·∫Øc ƒë∆°n gi·∫£n**, ch√∫ng ta ƒë√£ ƒë·∫°t ƒë∆∞·ª£c **ƒë·ªô ch√≠nh x√°c 70%**.  \n",
    "- N·∫øu ch√∫ng ta x·∫øp h·∫°ng m√¥ h√¨nh theo thang **T·ªá - K√©m - T·ªët - Kh√° - Xu·∫•t s·∫Øc**, th√¨ **70% s·∫Ω ƒë∆∞·ª£c x·∫øp v√†o m·ª©c \"T·ªët\"**.  \n",
    "- **Nh∆∞ng li·ªáu ch√∫ng ta c√≥ th·ªÉ l√†m t·ªët h∆°n m√¥ h√¨nh t·ª± t·∫°o n√†y kh√¥ng?** ü§î  \n",
    "\n",
    "---\n",
    "\n",
    "### **üìå Tr∆∞·ªõc khi ti·∫øp t·ª•c, h√£y vi·∫øt l·∫°i c√°c b∆∞·ªõc tr√™n th√†nh m√£ Python!**  \n",
    "\n",
    "H√£y nh·ªõ r·∫±ng, qu√° tr√¨nh tr√™n **ƒë∆∞·ª£c th·ª±c hi·ªán th·ªß c√¥ng**, nh∆∞ng ch√∫ng ta c√≥ th·ªÉ **t·ª± ƒë·ªông h√≥a** b·∫±ng c√°c thu·∫≠t to√°n MLA.  \n",
    "\n",
    "üîç **V√≠ MLA gi·ªëng nh∆∞ m·ªôt m√°y t√≠nh TI-89 trong k·ª≥ thi To√°n**.  \n",
    "- N√≥ **r·∫•t m·∫°nh m·∫Ω** v√† gi√∫p b·∫°n th·ª±c hi·ªán h·∫ßu h·∫øt c√°c c√¥ng vi·ªác t√≠nh to√°n.  \n",
    "- **Nh∆∞ng n·∫øu b·∫°n kh√¥ng hi·ªÉu b√†i to√°n**, th√¨ ngay c·∫£ m√°y t√≠nh m·∫°nh nh·∫•t c≈©ng **kh√¥ng th·ªÉ gi√∫p b·∫°n gi·∫£i b√†i**.  \n",
    "\n",
    "üìù V√¨ v·∫≠y, h√£y nghi√™n c·ª©u k·ªπ ph·∫ßn ti·∫øp theo tr∆∞·ªõc khi ƒëi s√¢u v√†o Machine Learning! üöÄ  \n",
    "\n",
    "üìñ **T√†i li·ªáu tham kh·∫£o:**  \n",
    "- [H∆∞·ªõng d·∫´n v·ªÅ Cross-Validation v√† Decision Tree](http://www.cs.utoronto.ca/~fidler/teaching/2015/slides/CSC411/tutorial3_CrossVal-DTs.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e6838-2d8b-4c1b-89dd-e43cac919596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT: This is a handmade model for learning purposes only.\n",
    "#However, it is possible to create your own predictive model without a fancy algorithm :)\n",
    "\n",
    "#coin flip model with random 1/survived 0/died\n",
    "\n",
    "#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\n",
    "for index, row in data1.iterrows(): \n",
    "    #random number generator: https://docs.python.org/2/library/random.html\n",
    "    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n",
    "        data1.at[index, 'Random_Predict'] = 1 # predict survived/1\n",
    "    else: \n",
    "        data1.at[index, 'Random_Predict'] = 0 # predict died/0\n",
    "    \n",
    "\n",
    "#score random guess of survival. Use shortcut 1 = Right Guess and 0 = Wrong Guess\n",
    "#the mean of the column will then equal the accuracy\n",
    "data1['Random_Score'] = 0 #assume prediction wrong\n",
    "data1.loc[(data1['Transported'] == data1['Random_Predict']), 'Random_Score'] = 1 #set to 1 for correct prediction\n",
    "print('Coin Flip Model Accuracy: {:.2f}%'.format(data1['Random_Score'].mean()*100))\n",
    "\n",
    "#we can also use scikit's accuracy_score function to save us a few lines of code\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "print('Coin Flip Model Accuracy w/SciKit: {:.2f}%'.format(metrics.accuracy_score(data1[Target], data1['Random_Predict'])*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da841568-2183-4ce5-b9f5-a9b7e05c9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec4ad7c-2ac8-43b6-95eb-25cb3df1a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\n",
    "pivot_age = data1.groupby(['Age_group'])['Transported'].mean()\n",
    "print('Survival Decision Tree age Node: \\n',pivot_age)\n",
    "\n",
    "pivot_vip = data1.groupby(['VIP_Code'])['Transported'].mean()\n",
    "print('\\n Survival Decision Tree vip Node: \\n',pivot_vip)\n",
    "\n",
    "pivot_HomePlanet = data1.groupby(['HomePlanet_Code'])['Transported'].mean()\n",
    "print('\\n Survival Decision Tree HomePlanet Node: \\n',pivot_HomePlanet)\n",
    "\n",
    "pivot_CryoSleep = data1.groupby(['CryoSleep_Code'])['Transported'].mean()\n",
    "print('\\n Survival Decision Tree CryoSleep Node: \\n',pivot_CryoSleep)\n",
    "\n",
    "pivot_Cabin_deck_Code = data1.groupby(['Cabin_deck_Code'])['Transported'].mean()\n",
    "print('\\n Survival Decision Tree Cabin_deck_Code Node: \\n',pivot_Cabin_deck_Code)\n",
    "\n",
    "pivot_Cabin_side_Code = data1.groupby(['Cabin_side_Code'])['Transported'].mean()\n",
    "print('\\n Survival Decision Tree Cabin_side_Code Node: \\n',pivot_Cabin_side_Code)\n",
    "\n",
    "# pivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Transported'].mean()\n",
    "# print('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14a645-fb29-4d3e-b1be-9b9b672e2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\n",
    "def mytree(df):\n",
    "    \n",
    "    #initialize table to store predictions\n",
    "    Model = pd.DataFrame(data = {'Predict':[]})\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        #Question 1: Age_group (55-69%)\n",
    "        if (df.loc[index, 'Age_group'] == 'Age_0-12') or (df.loc[index, 'Age_group'] == 'Age_13-17'):\n",
    "                  Model.loc[index, 'Predict'] = 1\n",
    "                \n",
    "        #Question 2: HomePlanet_Code (66-67%)\n",
    "        if (df.loc[index, 'HomePlanet_Code'] == 0):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "        if (df.loc[index, 'HomePlanet_Code'] == 1):\n",
    "                  Model.loc[index, 'Predict'] = 1 \n",
    "        \n",
    "        #Question 3: VIP_Code (71%)\n",
    "        if (df.loc[index, 'VIP_Code'] == 1):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "        \n",
    "        #Question 4: CryoSleep_Code (68-81%)\n",
    "        if (df.loc[index, 'CryoSleep_Code'] == 0):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "        if (df.loc[index, 'CryoSleep_Code'] == 1):\n",
    "                  Model.loc[index, 'Predict'] = 1  \n",
    "                \n",
    "        #Question 5: Cabin_deck_Code_Code (73-80%)\n",
    "        if (df.loc[index, 'Cabin_deck_Code'] == 7):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "        if (df.loc[index, 'Cabin_deck_Code'] == 1):\n",
    "                  Model.loc[index, 'Predict'] = 1\n",
    "        if (df.loc[index, 'Cabin_deck_Code'] == 2):\n",
    "                  Model.loc[index, 'Predict'] = 1\n",
    "        if (df.loc[index, 'Cabin_deck_Code'] == 4):\n",
    "                  Model.loc[index, 'Predict'] = 0                \n",
    "        \n",
    "        #Question 6: Cabin_side_Code (72%)\n",
    "        if (df.loc[index, 'Cabin_side_Code'] == 2):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "                \n",
    "        \n",
    "    return Model\n",
    "\n",
    "\n",
    "#model data\n",
    "Tree_Predict = mytree(data1)\n",
    "print('Decision Tree Model Accuracy/Precision Score: {:.2f}%\\n'.format(metrics.accuracy_score(data1['Transported'], Tree_Predict)*100))\n",
    "\n",
    "\n",
    "#Accuracy Summary Report with http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report\n",
    "#Where recall score = (true positives)/(true positive + false negative) w/1 being best:http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n",
    "#And F1 score = weighted average of precision and recall w/1 being best: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "print(metrics.classification_report(data1['Transported'], Tree_Predict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495eb74-2f08-4881-b640-3f49ffbea02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Accuracy Summary\n",
    "#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = metrics.confusion_matrix(data1['Transported'], Tree_Predict)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "class_names = ['NotTransported', 'Transported']\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, \n",
    "                      title='Normalized confusion matrix')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209a2c58-11ee-41a5-b4c1-de1baeb32a7d",
   "metadata": {},
   "source": [
    "## 5.11 Hi·ªáu su·∫•t M√¥ h√¨nh v·ªõi Cross-Validation (CV)\n",
    "\n",
    "·ªû b∆∞·ªõc 5.0, ch√∫ng ta ƒë√£ s·ª≠ d·ª•ng h√†m [`sklearn cross_validate`](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) ƒë·ªÉ hu·∫•n luy·ªán, ki·ªÉm tra v√† ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh.\n",
    "\n",
    "H√£y nh·ªõ r·∫±ng, ƒëi·ªÅu quan tr·ªçng l√† ch√∫ng ta ph·∫£i s·ª≠ d·ª•ng c√°c t·∫≠p d·ªØ li·ªáu kh√°c nhau cho qu√° tr√¨nh hu·∫•n luy·ªán v√† ki·ªÉm tra m√¥ h√¨nh. N·∫øu kh√¥ng, m√¥ h√¨nh s·∫Ω b·ªã **overfitting** (qu√° kh·ªõp). ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† m√¥ h√¨nh s·∫Ω ho·∫°t ƒë·ªông r·∫•t t·ªët tr√™n d·ªØ li·ªáu ƒë√£ th·∫•y tr∆∞·ªõc ƒë√≥, nh∆∞ng l·∫°i k√©m hi·ªáu qu·∫£ khi d·ª± ƒëo√°n tr√™n d·ªØ li·ªáu m·ªõi‚Äîv√† ƒë√≥ kh√¥ng ph·∫£i l√† d·ª± ƒëo√°n th·ª±c s·ª±. N√≥ gi·ªëng nh∆∞ vi·ªác gian l·∫≠n trong m·ªôt b√†i ki·ªÉm tra ·ªü tr∆∞·ªùng ƒë·ªÉ ƒë·∫°t ƒëi·ªÉm tuy·ªát ƒë·ªëi, nh∆∞ng khi thi th·∫≠t th√¨ l·∫°i tr∆∞·ª£t v√¨ th·ª±c ra ch∆∞a bao gi·ªù th·ª±c s·ª± hi·ªÉu b√†i. ƒêi·ªÅu t∆∞∆°ng t·ª± c≈©ng x·∫£y ra v·ªõi machine learning.\n",
    "\n",
    "**Cross-Validation (CV)** l√† m·ªôt c√°ch gi√∫p chia t·∫≠p d·ªØ li·ªáu th√†nh nhi·ªÅu ph·∫ßn v√† ƒë√°nh gi√° m√¥ h√¨nh nhi·ªÅu l·∫ßn. ƒêi·ªÅu n√†y gi√∫p ch√∫ng ta c√≥ c√°i nh√¨n t·ªïng quan h∆°n v·ªÅ hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh tr√™n d·ªØ li·ªáu ch∆∞a t·ª´ng th·∫•y. D√π t·ªën nhi·ªÅu t√†i nguy√™n t√≠nh to√°n h∆°n, nh∆∞ng ƒë√¢y l√† m·ªôt b∆∞·ªõc quan tr·ªçng ƒë·ªÉ tr√°nh c√≥ ƒë∆∞·ª£c s·ª± t·ª± tin sai l·∫ßm. ƒêi·ªÅu n√†y ƒë·∫∑c bi·ªát h·ªØu √≠ch trong c√°c cu·ªôc thi tr√™n Kaggle ho·∫∑c trong b·∫•t k·ª≥ tr∆∞·ªùng h·ª£p n√†o m√† t√≠nh ·ªïn ƒë·ªãnh v√† s·ª± ch√≠nh x√°c l√† y·∫øu t·ªë quan tr·ªçng.\n",
    "\n",
    "Ngo√†i CV, ch√∫ng ta c≈©ng s·ª≠ d·ª•ng m·ªôt phi√™n b·∫£n t√πy ch·ªânh c·ªßa [`sklearn train_test_split`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection), gi√∫p t·∫°o ra s·ª± ng·∫´u nhi√™n trong qu√° tr√¨nh ph√¢n chia d·ªØ li·ªáu ki·ªÉm tra. D∆∞·ªõi ƒë√¢y l√† h√¨nh ·∫£nh minh h·ªça v·ªÅ c√°ch chia m·∫∑c ƒë·ªãnh c·ªßa CV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f048437a-e251-4f22-902c-b3c26aa5c71e",
   "metadata": {},
   "source": [
    "<a id=\"ch9\"></a>\n",
    "# 5.12 ƒêi·ªÅu ch·ªânh M√¥ h√¨nh v·ªõi Hyper-Parameters\n",
    "\n",
    "Khi ch√∫ng ta s·ª≠ d·ª•ng [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), ch√∫ng ta ƒë√£ ch·∫•p nh·∫≠n t·∫•t c·∫£ c√°c gi√° tr·ªã m·∫∑c ƒë·ªãnh c·ªßa h√†m. ƒêi·ªÅu n√†y m·ªü ra c∆° h·ªôi ƒë·ªÉ xem c√°ch c√°c thi·∫øt l·∫≠p kh√°c nhau c·ªßa **hyper-parameter** s·∫Ω ·∫£nh h∆∞·ªüng ƒë·∫øn ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh. [(Nh·∫•n v√†o ƒë√¢y ƒë·ªÉ t√¨m hi·ªÉu th√™m v·ªÅ parameters v√† hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\n",
    "\n",
    "Tuy nhi√™n, ƒë·ªÉ ƒëi·ªÅu ch·ªânh m·ªôt m√¥ h√¨nh, tr∆∞·ªõc ti√™n ch√∫ng ta c·∫ßn th·ª±c s·ª± hi·ªÉu n√≥. ƒê√≥ l√† l√Ω do t·∫°i sao ·ªü c√°c ph·∫ßn tr∆∞·ªõc, t√¥i ƒë√£ d√†nh th·ªùi gian ƒë·ªÉ gi·∫£i th√≠ch c√°ch d·ª± ƒëo√°n ho·∫°t ƒë·ªông. B√¢y gi·ªù, h√£y c√πng t√¨m hi·ªÉu th√™m v·ªÅ thu·∫≠t to√°n **Decision Tree (DT)**.\n",
    "\n",
    "**Ngu·ªìn:** [sklearn](http://scikit-learn.org/stable/modules/tree.html#classification)\n",
    "\n",
    "## ‚úÖ **∆Øu ƒëi·ªÉm c·ªßa c√¢y quy·∫øt ƒë·ªãnh (Decision Tree):**\n",
    "- D·ªÖ hi·ªÉu v√† d·ªÖ di·ªÖn gi·∫£i. C√¢y quy·∫øt ƒë·ªãnh c√≥ th·ªÉ ƒë∆∞·ª£c tr·ª±c quan h√≥a.\n",
    "- Y√™u c·∫ßu r·∫•t √≠t c√¥ng t√°c chu·∫©n b·ªã d·ªØ li·ªáu. Trong khi c√°c ph∆∞∆°ng ph√°p kh√°c th∆∞·ªùng y√™u c·∫ßu chu·∫©n h√≥a d·ªØ li·ªáu, t·∫°o bi·∫øn gi·∫£ (dummy variables), v√† x·ª≠ l√Ω gi√° tr·ªã tr·ªëng th√¨ Decision Tree kh√¥ng c·∫ßn.\n",
    "- Chi ph√≠ t√≠nh to√°n th·∫•p (t·ªâ l·ªá logarit v·ªõi s·ªë ƒëi·ªÉm d·ªØ li·ªáu ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ hu·∫•n luy·ªán c√¢y).\n",
    "- C√≥ th·ªÉ x·ª≠ l√Ω c·∫£ d·ªØ li·ªáu s·ªë v√† d·ªØ li·ªáu ph√¢n lo·∫°i, trong khi nhi·ªÅu thu·∫≠t to√°n kh√°c ch·ªâ ho·∫°t ƒë·ªông t·ªët v·ªõi m·ªôt lo·∫°i d·ªØ li·ªáu.\n",
    "- C√≥ th·ªÉ x·ª≠ l√Ω c√°c b√†i to√°n nhi·ªÅu ƒë·∫ßu ra (multi-output).\n",
    "- L√† m·ªôt m√¥ h√¨nh **white box** (h·ªôp tr·∫Øng), d·ªÖ d√†ng gi·∫£i th√≠ch c√°c ƒëi·ªÅu ki·ªán c·ªßa m√¥ h√¨nh b·∫±ng logic Boolean. Ng∆∞·ª£c l·∫°i, c√°c m√¥ h√¨nh **black box** (h·ªôp ƒëen) nh∆∞ m·∫°ng n∆°-ron nh√¢n t·∫°o (ANN) c√≥ th·ªÉ r·∫•t kh√≥ di·ªÖn gi·∫£i.\n",
    "- C√≥ th·ªÉ ki·ªÉm tra ƒë·ªô tin c·∫≠y c·ªßa m√¥ h√¨nh b·∫±ng c√°c b√†i ki·ªÉm tra th·ªëng k√™.\n",
    "- Ho·∫°t ƒë·ªông t·ªët ngay c·∫£ khi c√°c gi·∫£ ƒë·ªãnh c·ªßa m√¥ h√¨nh kh√¥ng ho√†n to√†n ch√≠nh x√°c.\n",
    "\n",
    "## ‚ùå **Nh∆∞·ª£c ƒëi·ªÉm c·ªßa c√¢y quy·∫øt ƒë·ªãnh:**\n",
    "- C√≥ th·ªÉ t·∫°o ra c√°c c√¢y quy·∫øt ƒë·ªãnh qu√° ph·ª©c t·∫°p, kh√¥ng t·ªïng qu√°t h√≥a d·ªØ li·ªáu t·ªët (**overfitting**). ƒê·ªÉ kh·∫Øc ph·ª•c, c·∫ßn s·ª≠ d·ª•ng c√°c c∆° ch·∫ø nh∆∞ **pruning** (c·∫Øt t·ªâa), ƒë·∫∑t gi·ªõi h·∫°n s·ªë m·∫´u t·ªëi thi·ªÉu t·∫°i m·ªôt node l√°, ho·∫∑c ƒë·∫∑t ƒë·ªô s√¢u t·ªëi ƒëa c·ªßa c√¢y.\n",
    "- Nh·∫°y c·∫£m v·ªõi d·ªØ li·ªáu: Ch·ªâ c·∫ßn m·ªôt ch√∫t thay ƒë·ªïi trong t·∫≠p d·ªØ li·ªáu, c√¢y quy·∫øt ƒë·ªãnh c√≥ th·ªÉ thay ƒë·ªïi ho√†n to√†n.\n",
    "- B√†i to√°n t√¨m c√¢y quy·∫øt ƒë·ªãnh t·ªëi ∆∞u l√† **NP-complete**, do ƒë√≥ h·∫ßu h·∫øt c√°c thu·∫≠t to√°n hi·ªán nay ƒë·ªÅu d·ª±a tr√™n c√°c ph∆∞∆°ng ph√°p tham lam (**greedy algorithm**) ƒë·ªÉ t√¨m l·ªùi gi·∫£i c·ª•c b·ªô.\n",
    "- M·ªôt s·ªë lo·∫°i b√†i to√°n kh√≥ h·ªçc b·∫±ng c√¢y quy·∫øt ƒë·ªãnh, v√≠ d·ª• nh∆∞ b√†i to√°n **XOR**, **parity**, ho·∫∑c **multiplexer**.\n",
    "- C√≥ th·ªÉ t·∫°o ra c√°c c√¢y c√≥ ƒë·ªô ch·ªách cao n·∫øu m·ªôt s·ªë l·ªõp d·ªØ li·ªáu chi·∫øm ∆∞u th·∫ø. Do ƒë√≥, n√™n c√¢n b·∫±ng t·∫≠p d·ªØ li·ªáu tr∆∞·ªõc khi hu·∫•n luy·ªán m√¥ h√¨nh.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436796cd-6e36-4dd5-9c2c-b037acd67e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model\n",
    "dtree = tree.DecisionTreeClassifier(random_state = 0)\n",
    "base_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "dtree.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print('BEFORE DT Parameters: ', dtree.get_params())\n",
    "print(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "#print(\"BEFORE DT Test w/bin set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune hyper-parameters: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "param_grid = {'criterion': ['gini', 'entropy'],  #scoring methodology; two supported formulas for calculating information gain - default is gini\n",
    "              #'splitter': ['best', 'random'], #splitting methodology; two supported strategies - default is best\n",
    "              'max_depth': [2,4,6,8,10,None], #max depth tree can grow; default is none\n",
    "              #'min_samples_split': [2,5,10,.03,.05], #minimum subset size BEFORE new split (fraction is % of total); default is 2\n",
    "              #'min_samples_leaf': [1,5,10,.03,.05], #minimum subset size AFTER new split split (fraction is % of total); default is 1\n",
    "              #'max_features': [None, 'auto'], #max features to consider when performing split; default none or all\n",
    "              'random_state': [0] #seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n",
    "             }\n",
    "\n",
    "#print(list(model_selection.ParameterGrid(param_grid)))\n",
    "\n",
    "#choose best model with grid_search: #http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n",
    "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
    "tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True )\n",
    "tune_model.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "#print(tune_model.cv_results_.keys())\n",
    "#print(tune_model.cv_results_['params'])\n",
    "print('AFTER DT Parameters: ', tune_model.best_params_)\n",
    "#print(tune_model.cv_results_['mean_train_score'])\n",
    "print(\"AFTER DT Training w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(tune_model.cv_results_['mean_test_score'])\n",
    "print(\"AFTER DT Test w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT Test w/bin score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#duplicates gridsearchcv\n",
    "#tune_results = model_selection.cross_validate(tune_model, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
    "\n",
    "#print('AFTER DT Parameters: ', tune_model.best_params_)\n",
    "#print(\"AFTER DT Training w/bin set score mean: {:.2f}\". format(tune_results['train_score'].mean()*100)) \n",
    "#print(\"AFTER DT Test w/bin set score mean: {:.2f}\". format(tune_results['test_score'].mean()*100))\n",
    "#print(\"AFTER DT Test w/bin set score min: {:.2f}\". format(tune_results['test_score'].min()*100))\n",
    "#print('-'*10)\n",
    "#base model\n",
    "dtree = tree.DecisionTreeClassifier(random_state = 0)\n",
    "base_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "dtree.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print('BEFORE DT Parameters: ', dtree.get_params())\n",
    "print(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "#print(\"BEFORE DT Test w/bin set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune hyper-parameters: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "param_grid = {'criterion': ['gini', 'entropy'],  #scoring methodology; two supported formulas for calculating information gain - default is gini\n",
    "              #'splitter': ['best', 'random'], #splitting methodology; two supported strategies - default is best\n",
    "              'max_depth': [2,4,6,8,10,None], #max depth tree can grow; default is none\n",
    "              #'min_samples_split': [2,5,10,.03,.05], #minimum subset size BEFORE new split (fraction is % of total); default is 2\n",
    "              #'min_samples_leaf': [1,5,10,.03,.05], #minimum subset size AFTER new split split (fraction is % of total); default is 1\n",
    "              #'max_features': [None, 'auto'], #max features to consider when performing split; default none or all\n",
    "              'random_state': [0] #seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n",
    "             }\n",
    "\n",
    "#print(list(model_selection.ParameterGrid(param_grid)))\n",
    "\n",
    "#choose best model with grid_search: #http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n",
    "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
    "tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True )\n",
    "tune_model.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "#print(tune_model.cv_results_.keys())\n",
    "#print(tune_model.cv_results_['params'])\n",
    "print('AFTER DT Parameters: ', tune_model.best_params_)\n",
    "#print(tune_model.cv_results_['mean_train_score'])\n",
    "print(\"AFTER DT Training w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(tune_model.cv_results_['mean_test_score'])\n",
    "print(\"AFTER DT Test w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT Test w/bin score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#duplicates gridsearchcv\n",
    "#tune_results = model_selection.cross_validate(tune_model, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
    "\n",
    "#print('AFTER DT Parameters: ', tune_model.best_params_)\n",
    "#print(\"AFTER DT Training w/bin set score mean: {:.2f}\". format(tune_results['train_score'].mean()*100)) \n",
    "#print(\"AFTER DT Test w/bin set score mean: {:.2f}\". format(tune_results['test_score'].mean()*100))\n",
    "#print(\"AFTER DT Test w/bin set score min: {:.2f}\". format(tune_results['test_score'].min()*100))\n",
    "#print('-'*10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164b68d9-8510-486f-aba2-c734ae9cead1",
   "metadata": {},
   "source": [
    "<a id=\"ch10\"></a>  \n",
    "## 5.13 Tinh ch·ªânh m√¥ h√¨nh v·ªõi l·ª±a ch·ªçn ƒë·∫∑c tr∆∞ng  \n",
    "\n",
    "Nh∆∞ ƒë√£ ƒë·ªÅ c·∫≠p t·ª´ ƒë·∫ßu, c√≥ nhi·ªÅu bi·∫øn d·ª± ƒëo√°n h∆°n kh√¥ng c√≥ nghƒ©a l√† m√¥ h√¨nh s·∫Ω t·ªët h∆°n, m√† quan tr·ªçng l√† ch·ªçn ƒë√∫ng bi·∫øn d·ª± ƒëo√°n. V√¨ v·∫≠y, m·ªôt b∆∞·ªõc quan tr·ªçng trong qu√° tr√¨nh x√¢y d·ª±ng m√¥ h√¨nh l√† **l·ª±a ch·ªçn ƒë·∫∑c tr∆∞ng**.  \n",
    "\n",
    "[Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) cung c·∫•p nhi·ªÅu ph∆∞∆°ng ph√°p ƒë·ªÉ th·ª±c hi·ªán l·ª±a ch·ªçn ƒë·∫∑c tr∆∞ng. Trong ph·∫ßn n√†y, ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng **lo·∫°i b·ªè ƒë·∫∑c tr∆∞ng ƒë·ªá quy (Recursive Feature Elimination - RFE) k·∫øt h·ª£p v·ªõi Cross Validation (CV)** ƒë·ªÉ t·ªëi ∆∞u h√≥a vi·ªác l·ª±a ch·ªçn ƒë·∫∑c tr∆∞ng.  \n",
    "\n",
    "Tham kh·∫£o t√†i li·ªáu v·ªÅ RFE v·ªõi CV t·∫°i ƒë√¢y: [RFECV - Sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd40809-84f9-4f39-94db-d758207762da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model\n",
    "print('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \n",
    "print('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n",
    "\n",
    "print(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "\n",
    "#feature selection\n",
    "dtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split)\n",
    "dtree_rfe.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "#transform x&y to reduced features and fit new model\n",
    "#alternative: can use pipeline to reduce fit and transform steps: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "X_rfe = data1[data1_x_bin].columns.values[dtree_rfe.get_support()]\n",
    "rfe_results = model_selection.cross_validate(dtree, data1[X_rfe], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "\n",
    "#print(dtree_rfe.grid_scores_)\n",
    "print('AFTER DT RFE Training Shape New: ', data1[X_rfe].shape) \n",
    "print('AFTER DT RFE Training Columns New: ', X_rfe)\n",
    "\n",
    "print(\"AFTER DT RFE Training w/bin score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \n",
    "print(\"AFTER DT RFE Test w/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\n",
    "print(\"AFTER DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune rfe model\n",
    "rfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True )\n",
    "rfe_tune_model.fit(data1[X_rfe], data1[Target])\n",
    "\n",
    "#print(rfe_tune_model.cv_results_.keys())\n",
    "#print(rfe_tune_model.cv_results_['params'])\n",
    "print('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)\n",
    "#print(rfe_tune_model.cv_results_['mean_train_score'])\n",
    "print(\"AFTER DT RFE Tuned Training w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(rfe_tune_model.cv_results_['mean_test_score'])\n",
    "print(\"AFTER DT RFE Tuned Test w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT RFE Tuned Test w/bin score 3*std: +/- {:.2f}\". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79f49b-a5c0-4154-8f14-aa23f54dcf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
    "import graphviz \n",
    "dot_data = tree.export_graphviz(dtree, out_file=None, \n",
    "                                feature_names = data1_x_bin, class_names = True,\n",
    "                                filled = True, rounded = True)\n",
    "graph = graphviz.Source(dot_data) \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873e2bd5-0951-4c88-a5af-12381853c5a6",
   "metadata": {},
   "source": [
    "<a id=\"ch11\"></a>\n",
    "# B∆∞·ªõc 6: X√°c th·ª±c v√† Tri·ªÉn khai\n",
    "\n",
    "B∆∞·ªõc ti·∫øp theo l√† chu·∫©n b·ªã cho vi·ªác n·ªôp k·∫øt qu·∫£ b·∫±ng c√°ch s·ª≠ d·ª•ng d·ªØ li·ªáu ki·ªÉm tra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c1bcd7-4906-461e-99af-bc5647cc207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n",
    "#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\n",
    "correlation_heatmap(MLA_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d44386a-13e8-4b6a-8419-a0bbca9b82c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#why choose one model, when you can pick them all with voting classifier\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\n",
    "\n",
    "vote_est = [\n",
    "    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n",
    "    ('ada', ensemble.AdaBoostClassifier()),\n",
    "    ('bc', ensemble.BaggingClassifier()),\n",
    "    ('etc',ensemble.ExtraTreesClassifier()),\n",
    "    ('gbc', ensemble.GradientBoostingClassifier()),\n",
    "    ('rfc', ensemble.RandomForestClassifier()),\n",
    "\n",
    "    #Gaussian Processes: http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc\n",
    "    ('gpc', gaussian_process.GaussianProcessClassifier()),\n",
    "    \n",
    "    #GLM: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "    ('lr', linear_model.LogisticRegressionCV()),\n",
    "    \n",
    "    #Navies Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "    ('bnb', naive_bayes.BernoulliNB()),\n",
    "    ('gnb', naive_bayes.GaussianNB()),\n",
    "    \n",
    "    #Nearest Neighbor: http://scikit-learn.org/stable/modules/neighbors.html\n",
    "    ('knn', neighbors.KNeighborsClassifier()),\n",
    "    \n",
    "    #SVM: http://scikit-learn.org/stable/modules/svm.html\n",
    "    ('svc', svm.SVC(probability=True)),\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "   ('xgb', XGBClassifier())\n",
    "\n",
    "]\n",
    "\n",
    "#Hard Vote or majority rules\n",
    "vote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
    "vote_hard_cv = model_selection.cross_validate(vote_hard, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "vote_hard.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Hard Voting Training w/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \n",
    "print(\"Hard Voting Test w/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\n",
    "print(\"Hard Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "#Soft Vote or weighted probabilities\n",
    "vote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n",
    "vote_soft_cv = model_selection.cross_validate(vote_soft, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "vote_soft.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Soft Voting Training w/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \n",
    "print(\"Soft Voting Test w/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\n",
    "print(\"Soft Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f7660-b839-4dbd-bdfe-97795db245e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING: Running is very computational intensive and time expensive.\n",
    "#Code is written for experimental/developmental purposes and not production ready!\n",
    "\n",
    "#Hyperparameter Tune with GridSearchCV: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "grid_n_estimator = [10, 50, 100, 300]\n",
    "grid_ratio = [.1, .25, .5, .75, 1.0]\n",
    "grid_learn = [.01, .03, .05, .1, .25]\n",
    "grid_max_depth = [2, 4, 6, 8, 10, None]\n",
    "grid_min_samples = [5, 10, .03, .05, .10]\n",
    "grid_criterion = ['gini', 'entropy']\n",
    "grid_bool = [True, False]\n",
    "grid_seed = [0]\n",
    "\n",
    "\n",
    "grid_param = [\n",
    "            [{\n",
    "            #AdaBoostClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "            'n_estimators': grid_n_estimator, #default=50\n",
    "            'learning_rate': grid_learn, #default=1\n",
    "            #'algorithm': ['SAMME', 'SAMME.R'], #default=‚ÄôSAMME.R\n",
    "            'random_state': grid_seed\n",
    "            }],       \n",
    "    \n",
    "            [{\n",
    "            #BaggingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'max_samples': grid_ratio, #default=1.0\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{\n",
    "            #ExtraTreesClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'criterion': grid_criterion, #default=‚Äùgini‚Äù\n",
    "            'max_depth': grid_max_depth, #default=None\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "            [{\n",
    "            #GradientBoostingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n",
    "            #'loss': ['deviance', 'exponential'], #default=‚Äôdeviance‚Äô\n",
    "            'learning_rate': [.05], #default=0.1 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n",
    "            'n_estimators': [300], #default=100 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n",
    "            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=‚Äùfriedman_mse‚Äù\n",
    "            'max_depth': grid_max_depth, #default=3   \n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{\n",
    "            #RandomForestClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'criterion': grid_criterion, #default=‚Äùgini‚Äù\n",
    "            'max_depth': grid_max_depth, #default=None\n",
    "            'oob_score': [True], #default=False -- 12/31/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{    \n",
    "            #GaussianProcessClassifier\n",
    "            'max_iter_predict': grid_n_estimator, #default: 100\n",
    "            'random_state': grid_seed\n",
    "            }],        \n",
    "    \n",
    "            [{\n",
    "            #LogisticRegressionCV - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n",
    "            'fit_intercept': grid_bool, #default: True\n",
    "            #'penalty': ['l1','l2'],\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n",
    "            'random_state': grid_seed\n",
    "             }],            \n",
    "    \n",
    "            [{\n",
    "            #BernoulliNB - http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n",
    "            'alpha': grid_ratio, #default: 1.0\n",
    "             }],    \n",
    "    \n",
    "            #GaussianNB - \n",
    "            [{}],\n",
    "    \n",
    "            [{\n",
    "            #KNeighborsClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "            'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n",
    "            'weights': ['uniform', 'distance'], #default = ‚Äòuniform‚Äô\n",
    "            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "            }],            \n",
    "    \n",
    "            [{\n",
    "            #SVC - http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "            #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r\n",
    "            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'C': [1,2,3,4,5], #default=1.0\n",
    "            'gamma': grid_ratio, #edfault: auto\n",
    "            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n",
    "            'probability': [True],\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{\n",
    "            #XGBClassifier - http://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "            'learning_rate': grid_learn, #default: .3\n",
    "            'max_depth': [1,2,4,6,8,10], #default 2\n",
    "            'n_estimators': grid_n_estimator, \n",
    "            'seed': grid_seed  \n",
    "             }]   \n",
    "        ]\n",
    "\n",
    "start_total = time.perf_counter() #https://docs.python.org/3/library/time.html#time.perf_counter\n",
    "for clf, param in zip (vote_est, grid_param): #https://docs.python.org/3/library/functions.html#zip\n",
    "\n",
    "    #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm\n",
    "    #print(param)    \n",
    "    \n",
    "    start = time.perf_counter()        \n",
    "    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n",
    "    best_search.fit(data1[data1_x_bin], data1[Target])\n",
    "    run = time.perf_counter() - start\n",
    "\n",
    "    best_param = best_search.best_params_\n",
    "    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n",
    "    clf[1].set_params(**best_param) \n",
    "\n",
    "\n",
    "run_total = time.perf_counter() - start_total\n",
    "print('Total optimization time was {:.2f} minutes.'.format(run_total/60))\n",
    "\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54263ddc-f0cf-4bb7-ba3f-afb166aea58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hard Vote or majority rules w/Tuned Hyperparameters\n",
    "grid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
    "grid_hard_cv = model_selection.cross_validate(grid_hard, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True)\n",
    "grid_hard.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Hard Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \n",
    "print(\"Hard Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\n",
    "print(\"Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "#Soft Vote or weighted probabilities w/Tuned Hyperparameters\n",
    "grid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n",
    "grid_soft_cv = model_selection.cross_validate(grid_soft, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True)\n",
    "grid_soft.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Soft Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \n",
    "print(\"Soft Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\n",
    "print(\"Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d06211-537c-473a-ae7c-4680c5c8dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for modeling\n",
    "print(data_val.info())\n",
    "print(\"-\"*10)\n",
    "#data_val.sample(10)\n",
    "\n",
    "#handmade decision tree - submission score = 0.77990\n",
    "# data_val['Transported'] = mytree(data_val).astype(int)  # 0 V7\n",
    "data_val['Transported'] = mytree(data_val)\n",
    "\n",
    "#decision tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n",
    "#submit_dt = tree.DecisionTreeClassifier()\n",
    "#submit_dt = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_dt.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_dt.best_params_) #Best Parameters:  {'criterion': 'gini', 'max_depth': 4, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_dt.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\n",
    "#submit_bc = ensemble.BaggingClassifier()\n",
    "#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_bc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n",
    "#submit_etc = ensemble.ExtraTreesClassifier()\n",
    "#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_etc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\n",
    "#submit_rfc = ensemble.RandomForestClassifier()\n",
    "#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_rfc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\n",
    "#submit_abc = ensemble.AdaBoostClassifier()\n",
    "#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_abc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\n",
    "#submit_gbc = ensemble.GradientBoostingClassifier()\n",
    "#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_gbc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\n",
    "\n",
    "#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\n",
    "#submit_xgb = XGBClassifier()\n",
    "#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_xgb.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\n",
    "#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#hard voting classifier w/full dataset modeling submission score: defaults=-, tuned = 0.74655 V4\n",
    "# data_val['Transported'] = vote_hard.predict(data_val[data1_x_bin])  # 0.74655 V4\n",
    "# data_val['Transported'] = grid_hard.predict(data_val[data1_x_bin])  # 0.70189 V4\n",
    "\n",
    "\n",
    "#soft voting classifier w/full dataset modeling submission score: defaults=-, tuned = 0.75005 V6\n",
    "# data_val['Transported'] = vote_soft.predict(data_val[data1_x_bin])  # 0.75005 V6\n",
    "# data_val['Transported'] = grid_soft.predict(data_val[data1_x_bin])  # 0.74982 V5\n",
    "\n",
    "\n",
    "#submit file\n",
    "submit = data_val[['PassengerId','Transported']]\n",
    "submit.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print('Validation Data Distribution: \\n', data_val['Transported'].value_counts(normalize = True))\n",
    "submit.sample(10)\n",
    "\n",
    "# The best parameter for AdaBoostClassifier is {'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0} with a runtime of 154.92 seconds.\n",
    "# The best parameter for BaggingClassifier is {'max_samples': 0.1, 'n_estimators': 300, 'random_state': 0} with a runtime of 269.41 seconds.\n",
    "# The best parameter for ExtraTreesClassifier is {'criterion': 'gini', 'max_depth': 8, 'n_estimators': 300, 'random_state': 0} with a runtime of 284.94 seconds.\n",
    "# The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 300, 'random_state': 0} with a runtime of 497.71 seconds.\n",
    "# The best parameter for RandomForestClassifier is {'criterion': 'gini', 'max_depth': 8, 'n_estimators': 300, 'oob_score': True, 'random_state': 0} with a runtime of 396.49 seconds.\n",
    "# The best parameter for GaussianProcessClassifier is {'max_iter_predict': 10, 'random_state': 0} with a runtime of 1470.09 seconds.\n",
    "# The best parameter for LogisticRegressionCV is {'fit_intercept': True, 'random_state': 0, 'solver': 'lbfgs'} with a runtime of 445.46 seconds.\n",
    "# The best parameter for BernoulliNB is {'alpha': 0.5} with a runtime of 1.12 seconds.\n",
    "# The best parameter for GaussianNB is {} with a runtime of 0.26 seconds.\n",
    "# The best parameter for KNeighborsClassifier is {'algorithm': 'ball_tree', 'n_neighbors': 7, 'weights': 'distance'} with a runtime of 47.07 seconds.\n",
    "# The best parameter for SVC is {'C': 1, 'decision_function_shape': 'ovo', 'gamma': 0.1, 'probability': True, 'random_state': 0} with a runtime of 5298.85 seconds.\n",
    "# The best parameter for XGBClassifier is {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 50, 'seed': 0} with a runtime of 614.08 seconds.\n",
    "# Total optimization time was 158.01 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d5efb-6d7b-4e10-9502-b7c44f717d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e5de6-fcec-4f50-a58e-1a1d64d5ab1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5efe86c-9e8f-4cd7-b4fb-0994a3d7effe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8277eab9-654d-407d-850e-fa2a9c9282a8",
   "metadata": {},
   "source": [
    "## Ghi chú:\n",
    "- Những bước cần làm khi làm một mô hình máy học:\n",
    "  * Bước 1: Xác định vấn đề\n",
    "  * Bước 2: Thu nhập dữ liệu\n",
    "  * Bước 3: Chuẩn bị sữ lí dữ liệu\n",
    "  * Bước 4: Phân tich dữ liệu thăm dò bằng thống kê\n",
    "  * Bước 5: Xây dựng mô hình dữ liệu\n",
    "  * Bước 6: Đánh giá mô hình\n",
    "  * Bước 7: Tinh chỉnh mô hình bằng siêu tham số\n",
    "  * Bước 8: Tinh chỉnh mô hình bằng chọn lọc đặc trưng\n",
    "  * Bước 9: Xác thực và triển khai\n",
    "  * Bước 10: Kết luận tối ưu hóa và chiến lược"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f337e1-5cb0-46cd-9a05-016d91cd619d",
   "metadata": {},
   "source": [
    "# Bước 3: Chuẩn bị sữ lí dữ liệu\n",
    "## Bước 3.1: Import thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7d6c68-d9fa-475f-af8e-c55eff6e5a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load Thư viện\n",
    "import sys  # Kiểm tra phiên bản Python\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "\n",
    "import pandas as pd # Xử lý dữ liệu dạng bảng (DataFrame).\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "import matplotlib # Vẽ biểu đồ khoa học\n",
    "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
    "\n",
    "import numpy as np # Tính toán khoa học, xử lý mảng số học\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "\n",
    "import scipy as sp # Công cụ toán học nâng cao\n",
    "print(\"SciPy version: {}\". format(sp.__version__)) \n",
    "\n",
    "import IPython\n",
    "from IPython import display # Hiển thị dữ liệu đẹp hơn trong Jupyter Notebook\n",
    "print(\"IPython version: {}\". format(IPython.__version__)) \n",
    "\n",
    "import sklearn # Các thuật toán Machine Learning\n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
    "\n",
    "import random # Làm việc với các số ngẫu nhiên\n",
    "import time # Xử lý thời gian\n",
    "\n",
    "\n",
    "# Tắt cảnh báo không quan trọng\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('-'*25)\n",
    "\n",
    "\n",
    "\n",
    "# Chương trình kiểm tra danh sách tệp trong thư mục chứa dữ liệu bằng lệnh:\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "719e4422-d46b-4bd8-b95c-949b8f07663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4dd21-df33-4136-a589-a91eb5fc3999",
   "metadata": {},
   "source": [
    "### Các thuật toán phổ biến được sử dụng:\n",
    "- ✅ `svm` → Support Vector Machines (SVM).\n",
    "- ✅ `tree` → Decision Tree Classifier.\n",
    "- ✅ `linear_model` → Các mô hình hồi quy tuyến tính (Logistic Regression).\n",
    "- ✅ `neighbors` → K-Nearest Neighbors (KNN).\n",
    "- ✅ `naive_bayes` → Naive Bayes Classifier.\n",
    "- ✅ `ensemble` → Các mô hình ensemble như Random Forest, Gradient Boosting.\n",
    "- ✅ `discriminant_analysis` → Linear Discriminant Analysis (LDA).\n",
    "- ✅ `gaussian_process` → Gaussian Process Classifier.\n",
    "- ✅ `XGBClassifier` từ xgboost → Một mô hình boosting mạnh mẽ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2dbd5da-864f-49c7-ba35-2f131b8b7293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection, model_selection, metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d012956-649b-4ce3-ab3d-7d6d8dd1ba24",
   "metadata": {},
   "source": [
    "### Các công cụ giúp xử lý dữ liệu đầu vào:\n",
    "- ✅ `OneHotEncoder` & `LabelEncoder` → Mã hóa biến phân loại (categorical variables).-\n",
    "- ✅ `feature_selection` → Chọn các đặc trưng quan trọng nhất.\n",
    "- ✅ `model_selection` → Chia tập dữ liệu (train/test split), k-fold cross-validation.\n",
    "- ✅ `metrics` → Các phép đánh giá mô hình (accuracy, F1-score, confusion matrix).\n",
    "- ✅ `Pipeline` → Tạo pipeline giúp chuẩn hóa và huấn luyện mô hình dễ dàng hơn.\n",
    "- ✅ `ColumnTransformer` → Dùng để xử lý các cột dữ liệu khác nhau theo cách khác nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167546b-709d-48b4-8109-9ccd3d3c70c7",
   "metadata": {},
   "source": [
    "## 3.2 Khám phá dữ liệu (Meet and Greet Data)\n",
    "### Mục tiêu:\n",
    "- Kiểm tra sơ bộ dữ liệu bằng cách import và quan sát.\n",
    "- Nhận diện các biến đầu vào (feature variables) và biến mục tiêu (target variable).\n",
    "- Xác định các kiểu dữ liệu (số, chuỗi, boolean).\n",
    "- Kiểm tra thông tin thiếu dữ liệu\n",
    "### Ví dụ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69b2de85-087a-44a4-a3dd-6e18434dab2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8693 entries, 0 to 8692\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   PassengerId   8693 non-null   object \n",
      " 1   HomePlanet    8492 non-null   object \n",
      " 2   CryoSleep     8476 non-null   object \n",
      " 3   Cabin         8494 non-null   object \n",
      " 4   Destination   8511 non-null   object \n",
      " 5   Age           8514 non-null   float64\n",
      " 6   VIP           8490 non-null   object \n",
      " 7   RoomService   8512 non-null   float64\n",
      " 8   FoodCourt     8510 non-null   float64\n",
      " 9   ShoppingMall  8485 non-null   float64\n",
      " 10  Spa           8510 non-null   float64\n",
      " 11  VRDeck        8505 non-null   float64\n",
      " 12  Name          8493 non-null   object \n",
      " 13  Transported   8693 non-null   bool   \n",
      "dtypes: bool(1), float64(6), object(7)\n",
      "memory usage: 891.5+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2192</th>\n",
       "      <td>2347_04</td>\n",
       "      <td>Earth</td>\n",
       "      <td>True</td>\n",
       "      <td>G/377/P</td>\n",
       "      <td>PSO J318.5-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Joanry Wellierras</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>1474_01</td>\n",
       "      <td>Mars</td>\n",
       "      <td>True</td>\n",
       "      <td>F/287/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>32.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Morms Melte</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6208</th>\n",
       "      <td>6563_02</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>G/1067/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>30.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>784.0</td>\n",
       "      <td>Juanna Gainney</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4253</th>\n",
       "      <td>4524_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/930/P</td>\n",
       "      <td>PSO J318.5-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Briane Fulloydez</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId HomePlanet CryoSleep     Cabin    Destination   Age    VIP  \\\n",
       "2192     2347_04      Earth      True   G/377/P  PSO J318.5-22   0.0  False   \n",
       "1400     1474_01       Mars      True   F/287/P    TRAPPIST-1e  32.0  False   \n",
       "6208     6563_02      Earth     False  G/1067/S    TRAPPIST-1e  30.0  False   \n",
       "4253     4524_01      Earth     False   F/930/P  PSO J318.5-22   NaN    NaN   \n",
       "\n",
       "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck               Name  \\\n",
       "2192          0.0        0.0           0.0  0.0     0.0  Joanry Wellierras   \n",
       "1400          0.0        0.0           0.0  NaN     0.0        Morms Melte   \n",
       "6208          0.0      123.0           0.0  1.0   784.0     Juanna Gainney   \n",
       "4253          0.0      116.0        2048.0  2.0     0.0   Briane Fulloydez   \n",
       "\n",
       "      Transported  \n",
       "2192         True  \n",
       "1400         True  \n",
       "6208        False  \n",
       "4253         True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Đọc file dữ liệu từ Kaggle\n",
    "data_raw = pd.read_csv('./data/train.csv')\n",
    "data_val = pd.read_csv('./data/test.csv')\n",
    "\n",
    "# Sao chép dữ liệu để làm sạch mà không ảnh hưởng đến bản gốc\n",
    "data1 = data_raw.copy(deep=True)\n",
    "\n",
    "# Danh sách chứa cả tập train và test (tiện xử lý dữ liệu)\n",
    "data_cleaner = [data1, data_val]\n",
    "\n",
    "# Hiển thị thông tin tổng quan về dữ liệu\n",
    "print(data_raw.info()) \n",
    "\n",
    "# Hiển thị 10 dòng ngẫu nhiên trong tập dữ liệu\n",
    "data_raw.sample(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bbcfa2e-91a9-4f51-869d-c6c59611151f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates in train set: 0, (0.0%)\n",
      "\n",
      "Duplicates in test set: 0, (0.0%)\n"
     ]
    }
   ],
   "source": [
    "#  Kiểu tra trùng dữ liệu\n",
    "print(f'Duplicates in train set: {data_raw.duplicated().sum()}, ({np.round(100*data_raw.duplicated().sum()/len(data_raw),1)}%)')\n",
    "print('')\n",
    "print(f'Duplicates in test set: {data_val.duplicated().sum()}, ({np.round(100*data_val.duplicated().sum()/len(data_val),1)}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f755c5d-27e3-4666-b8a6-2ad1d9cf9039",
   "metadata": {},
   "source": [
    "#### 3.21 Tóm tắt 4 bước làm sạch dữ liệu (4 C's of Data Cleaning)\n",
    "- **1.Correction (Chỉnh sửa dữ liệu lỗi, ngoại lệ)**:\n",
    "    * Xác định và sửa các giá trị bất thường (outlier).\n",
    "    * Ví dụ: Nếu có hành khách có tuổi 800 thay vì 80, cần sửa lại hoặc loại bỏ.\n",
    "    * Tránh chỉnh sửa dữ liệu gốc trừ khi có lý do rõ ràng.\n",
    "- **2.Completing (Hoàn thiện dữ liệu bị thiếu - xử lý missing values)**:\n",
    "    * Một số thuật toán không xử lý được giá trị rỗng (NaN), nên cần điền giá trị hợp lý.\n",
    "    * Dữ liệu định tính (categorical) → Thường điền bằng mode (giá trị xuất hiện nhiều nhất).\n",
    "    * Dữ liệu định lượng (numerical) → Điền bằng mean (trung bình), median (trung vị) hoặc trung bình + độ lệch chuẩn ngẫu nhiên.\n",
    "    * Ví dụ:\n",
    "    * Cột Age → Điền giá trị median (trung vị) để tránh ảnh hưởng bởi outlier.\n",
    "    * Cột HomePlanet → Điền bằng mode (hành tinh phổ biến nhất).\n",
    "- **3.Creating (Tạo đặc trưng mới - Feature Engineering)**:\n",
    "    * Dùng các cột hiện có để tạo ra đặc trưng mới giúp cải thiện mô hình.\n",
    "    * Ví dụ:\n",
    "    * Cột Cabin có dạng deck/num/side, có thể tách thành 3 cột riêng (Deck, Num, Side).\n",
    "    * Xác định xem họ của hành khách có ảnh hưởng đến việc được chọn không.\n",
    "- **4.Converting (Chuyển đổi dữ liệu về dạng phù hợp)**:\n",
    "    * Chuyển đổi dữ liệu về đúng kiểu:\n",
    "    * Categorical (chuỗi văn bản) → Chuyển thành dummies (one-hot encoding) để dễ tính toán.\n",
    "    * Boolean (True/False) → Chuyển thành 0/1.\n",
    "    * Ví dụ:\n",
    "    * Cột CryoSleep (True/False) → Đổi thành 0/1.\n",
    "    * Cột HomePlanet → Ánh xạ các hành tinh thành số hoặc dùng One-Hot Encoding.\n",
    "- 💡 Mục tiêu: Làm sạch, chuẩn bị dữ liệu tốt hơn để cải thiện độ chính xác của mô hình AI/ML. 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe70c257-342c-4ad0-b667-d0f543ebb1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns with null values:\n",
      " PassengerId       0\n",
      "HomePlanet      201\n",
      "CryoSleep       217\n",
      "Cabin           199\n",
      "Destination     182\n",
      "Age             179\n",
      "VIP             203\n",
      "RoomService     181\n",
      "FoodCourt       183\n",
      "ShoppingMall    208\n",
      "Spa             183\n",
      "VRDeck          188\n",
      "Name            200\n",
      "Transported       0\n",
      "dtype: int64\n",
      "----------\n",
      "Test/Validation columns with null values:\n",
      " PassengerId       0\n",
      "HomePlanet       87\n",
      "CryoSleep        93\n",
      "Cabin           100\n",
      "Destination      92\n",
      "Age              91\n",
      "VIP              93\n",
      "RoomService      82\n",
      "FoodCourt       106\n",
      "ShoppingMall     98\n",
      "Spa             101\n",
      "VRDeck           80\n",
      "Name             94\n",
      "dtype: int64\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8693</td>\n",
       "      <td>8492</td>\n",
       "      <td>8476</td>\n",
       "      <td>8494</td>\n",
       "      <td>8511</td>\n",
       "      <td>8514.000000</td>\n",
       "      <td>8490</td>\n",
       "      <td>8512.000000</td>\n",
       "      <td>8510.000000</td>\n",
       "      <td>8485.000000</td>\n",
       "      <td>8510.000000</td>\n",
       "      <td>8505.000000</td>\n",
       "      <td>8493</td>\n",
       "      <td>8693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8693</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6560</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8473</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>0001_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>G/734/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gollux Reedall</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>4602</td>\n",
       "      <td>5439</td>\n",
       "      <td>8</td>\n",
       "      <td>5915</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>4378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.827930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>224.687617</td>\n",
       "      <td>458.077203</td>\n",
       "      <td>173.729169</td>\n",
       "      <td>311.138778</td>\n",
       "      <td>304.854791</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.489021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>666.717663</td>\n",
       "      <td>1611.489240</td>\n",
       "      <td>604.696458</td>\n",
       "      <td>1136.705535</td>\n",
       "      <td>1145.717189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14327.000000</td>\n",
       "      <td>29813.000000</td>\n",
       "      <td>23492.000000</td>\n",
       "      <td>22408.000000</td>\n",
       "      <td>24133.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId HomePlanet CryoSleep    Cabin  Destination          Age  \\\n",
       "count         8693       8492      8476     8494         8511  8514.000000   \n",
       "unique        8693          3         2     6560            3          NaN   \n",
       "top        0001_01      Earth     False  G/734/S  TRAPPIST-1e          NaN   \n",
       "freq             1       4602      5439        8         5915          NaN   \n",
       "mean           NaN        NaN       NaN      NaN          NaN    28.827930   \n",
       "std            NaN        NaN       NaN      NaN          NaN    14.489021   \n",
       "min            NaN        NaN       NaN      NaN          NaN     0.000000   \n",
       "25%            NaN        NaN       NaN      NaN          NaN    19.000000   \n",
       "50%            NaN        NaN       NaN      NaN          NaN    27.000000   \n",
       "75%            NaN        NaN       NaN      NaN          NaN    38.000000   \n",
       "max            NaN        NaN       NaN      NaN          NaN    79.000000   \n",
       "\n",
       "          VIP   RoomService     FoodCourt  ShoppingMall           Spa  \\\n",
       "count    8490   8512.000000   8510.000000   8485.000000   8510.000000   \n",
       "unique      2           NaN           NaN           NaN           NaN   \n",
       "top     False           NaN           NaN           NaN           NaN   \n",
       "freq     8291           NaN           NaN           NaN           NaN   \n",
       "mean      NaN    224.687617    458.077203    173.729169    311.138778   \n",
       "std       NaN    666.717663   1611.489240    604.696458   1136.705535   \n",
       "min       NaN      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       NaN      0.000000      0.000000      0.000000      0.000000   \n",
       "50%       NaN      0.000000      0.000000      0.000000      0.000000   \n",
       "75%       NaN     47.000000     76.000000     27.000000     59.000000   \n",
       "max       NaN  14327.000000  29813.000000  23492.000000  22408.000000   \n",
       "\n",
       "              VRDeck            Name Transported  \n",
       "count    8505.000000            8493        8693  \n",
       "unique           NaN            8473           2  \n",
       "top              NaN  Gollux Reedall        True  \n",
       "freq             NaN               2        4378  \n",
       "mean      304.854791             NaN         NaN  \n",
       "std      1145.717189             NaN         NaN  \n",
       "min         0.000000             NaN         NaN  \n",
       "25%         0.000000             NaN         NaN  \n",
       "50%         0.000000             NaN         NaN  \n",
       "75%        46.000000             NaN         NaN  \n",
       "max     24133.000000             NaN         NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xác định cột nào có dữ liệu bị thiếu để xử lý.\n",
    "print('Train columns with null values:\\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values:\\n', data_val.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "data_raw.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49a38494-b39a-402a-93b6-b6a61bf62b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId     8693\n",
       "HomePlanet         3\n",
       "CryoSleep          2\n",
       "Cabin           6560\n",
       "Destination        3\n",
       "Age               80\n",
       "VIP                2\n",
       "RoomService     1273\n",
       "FoodCourt       1507\n",
       "ShoppingMall    1115\n",
       "Spa             1327\n",
       "VRDeck          1306\n",
       "Name            8473\n",
       "Transported        2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc9674-47c8-439e-a2a9-f34cf8309edc",
   "metadata": {},
   "source": [
    "- Continuous Features (6 đặc trưng liên tục - số thực hoặc số nguyên)\n",
    "- Categorical Features (4 đặc trưng phân loại - rời rạc)\n",
    "- Descriptive/Qualitative Features (3 đặc trưng mô tả/định tính - không trực tiếp dùng để dự đoán\n",
    "\n",
    "- Mục đích của việc phân loại này:\n",
    "    * **Các đặc trưng liên tục** (Continuous) có thể được chuẩn hóa hoặc xử lý outlier.\n",
    "    * **Các đặc trưng phân loại** (Categorical) cần được mã hóa (Encoding) để đưa vào mô hình.\n",
    "    * **Các đặc trưng mô tả** (Descriptive) có thể được loại bỏ hoặc tách thành thông tin hữu ích hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "728746d9-fc30-4617-96fc-4d56fec58cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expenditure features\n",
    "exp_feats=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "# Categorical features\n",
    "cat_feats=['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\n",
    "\n",
    "# Qualitative features\n",
    "qual_feats=['PassengerId', 'Cabin' ,'Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d6f47-93fe-4aa9-9e03-534f9c7bd62f",
   "metadata": {},
   "source": [
    "### 3.22 Làm sạch dữ liệu\n",
    "** Developer Documentation: **\n",
    "* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n",
    "* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n",
    "* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n",
    "* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-docs/stable/indexing.html)\n",
    "* [pandas.isnull](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html)\n",
    "* [pandas.DataFrame.sum](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sum.html)\n",
    "* [pandas.DataFrame.mode](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mode.html)\n",
    "* [pandas.DataFrame.copy](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.copy.html)\n",
    "* [pandas.DataFrame.fillna](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html)\n",
    "* [pandas.DataFrame.drop](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html)\n",
    "* [pandas.Series.value_counts](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html)\n",
    "* [pandas.DataFrame.loc](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd796fa-0c9a-48a7-99ca-57ec0bd4b791",
   "metadata": {},
   "source": [
    "##### Ghi chú:\n",
    "- sns: countPlot,histPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b81661-ce55-45f7-9129-728ccc68155e",
   "metadata": {},
   "source": [
    "#### 3.23 Mã hóa dữ liệu\n",
    "- Chúng tôi sẽ chuyển đổi dữ liệu phân loại thành các biến giả để phân tích toán học. Có nhiều cách để mã hóa các biến phân loại; chúng tôi sẽ sử dụng các hàm sklearn và pandas\n",
    "-  **Developer Documentation:**\n",
    "* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n",
    "* [Sklearn LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "* [Sklearn OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "* [Pandas Categorical dtype](https://pandas.pydata.org/pandas-docs/stable/categorical.html)\n",
    "* [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b0cd7e-6f63-4f25-96b9-25ecd588c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n",
    "\n",
    "#code categorical data\n",
    "label = LabelEncoder()\n",
    "\n",
    "for data in data_cleaner:\n",
    "\n",
    "    data['HomePlanet_Code'] = label.fit_transform(data['HomePlanet'])\n",
    "    data['CryoSleep_Code'] = label.fit_transform(data['CryoSleep'])\n",
    "    data['Destination_Code'] = label.fit_transform(data['Destination'])\n",
    "    data['VIP_Code'] = label.fit_transform(data['VIP'])\n",
    "    data['Age_group_Code'] = label.fit_transform(data['Age_group'])\n",
    "    data['Cabin_deck_Code'] = label.fit_transform(data['Cabin_deck'])\n",
    "    data['Cabin_side_Code'] = label.fit_transform(data['Cabin_side'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d968e92-7653-4636-ad9d-c8177ddae90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define y variable aka target/outcome\n",
    "Target = ['Transported']\n",
    "\n",
    "# define x variables for original features aka feature selection\n",
    "\n",
    "# 'HomePlanet', 'CryoSleep', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'\n",
    "data1_x = ['HomePlanet', 'CryoSleep', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'] # Original data\n",
    "# 'HomePlanet', 'CryoSleep', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Transported', 'Age_group', 'Expenditure', 'No_spending', 'Group', 'Group_size', 'Solo', 'Cabin_deck', 'Cabin_number', 'Cabin_side', 'Cabin_region1', 'Cabin_region2', 'Cabin_region3', 'Cabin_region4', 'Cabin_region5', 'Cabin_region6', 'Cabin_region7', 'Family_size', 'HomePlanet_Code', 'CryoSleep_Code', 'Destination_Code', 'VIP_Code', 'Age_group_Code', 'Cabin_deck_Code', 'Cabin_side_Code'\n",
    "\n",
    "data1_x_calc = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Expenditure', 'No_spending', 'Group', 'Group_size', 'Solo', 'Cabin_number', 'Cabin_region1', 'Cabin_region2', 'Cabin_region3', 'Cabin_region4', 'Cabin_region5', 'Cabin_region6', 'Cabin_region7', 'Family_size', 'HomePlanet_Code', 'CryoSleep_Code', 'Destination_Code', 'VIP_Code', 'Age_group_Code', 'Cabin_deck_Code', 'Cabin_side_Code'] # coded for algorithm calculation\n",
    "data1_xy =  Target + data1_x\n",
    "print('Original X Y: ', data1_xy, '\\n')\n",
    "\n",
    "\n",
    "# define x variables for original w/bin features to remove continuous variables\n",
    "data1_x_bin = ['Age', 'No_spending', 'Group_size', 'Solo', 'Cabin_region1', 'Cabin_region2', 'Cabin_region3', 'Cabin_region4', 'Cabin_region5', 'Cabin_region6', 'Cabin_region7', 'Family_size', 'HomePlanet_Code', 'CryoSleep_Code', 'Destination_Code', 'VIP_Code', 'Age_group_Code', 'Cabin_deck_Code', 'Cabin_side_Code']\n",
    "data1_xy_bin = Target + data1_x_bin\n",
    "print('Bin X Y: ', data1_xy_bin, '\\n')\n",
    "\n",
    "\n",
    "#define x and y variables for dummy features original\n",
    "data1_dummy = pd.get_dummies(data1[data1_x])\n",
    "data1_x_dummy = data1_dummy.columns.tolist()\n",
    "data1_xy_dummy = Target + data1_x_dummy\n",
    "print('Dummy X Y: ', data1_xy_dummy, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "data1_dummy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e0016-e476-4d2a-9fee-c4173585d899",
   "metadata": {},
   "source": [
    "## 3.24 Kiểm tra dữ liệu đã làm sạch Da-Double\n",
    "\n",
    "Bây giờ chúng ta đã làm sạch dữ liệu, hãy thực hiện kiểm tra da-double!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bbe83-97dd-4149-bbdb-ca46e2702bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train columns with null values: \\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (data1.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values: \\n', data_val.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (data_val.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "data_raw.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0019fbf9-9aea-4538-af75-2cd2fd17da2d",
   "metadata": {},
   "source": [
    "## 3.25 Phân chia dữ liệu đào tạo và thử nghiệm\n",
    "\n",
    "Như đã đề cập trước đó, tệp thử nghiệm được cung cấp thực sự là dữ liệu xác thực để gửi bài dự thi. Vì vậy, chúng ta sẽ sử dụng hàm *sklearn* để phân chia dữ liệu đào tạo thành hai tập dữ liệu; chia 75/25. Điều này rất quan trọng, vì vậy chúng ta không [quá phù hợp với mô hình của mình](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Nghĩa là, thuật toán quá cụ thể đối với một tập hợp con nhất định, nên nó không thể khái quát chính xác một tập hợp con khác, từ cùng một tập dữ liệu. Điều quan trọng là thuật toán của chúng ta chưa nhìn thấy tập hợp con mà chúng ta sẽ sử dụng để thử nghiệm, vì vậy nó không \"gian lận\" bằng cách ghi nhớ các câu trả lời. Chúng ta sẽ sử dụng hàm train_test_split của [*sklearn*](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). Trong các phần sau, chúng ta cũng sẽ sử dụng các hàm xác thực chéo của [*sklearn*](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation), chia tập dữ liệu của chúng ta thành tập huấn luyện và tập kiểm tra để so sánh mô hình dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0883958-be5d-44b7-a194-39f3be3a7507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train and test data with function defaults\n",
    "#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n",
    "train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\n",
    "train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\n",
    "train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)\n",
    "\n",
    "\n",
    "print(\"Data1 Shape: {}\".format(data1.shape))\n",
    "print(\"Train1 Shape: {}\".format(train1_x.shape))\n",
    "print(\"Test1 Shape: {}\".format(test1_x.shape))\n",
    "\n",
    "train1_x_bin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15410809-8615-47a7-bb85-a8d0b118f699",
   "metadata": {},
   "source": [
    "<a id=\"ch6\"></a>\n",
    "# Bước 4: Thực hiện Phân tích thăm dò với Thống kê\n",
    "Bây giờ dữ liệu của chúng ta đã được làm sạch, chúng ta sẽ khám phá dữ liệu của mình bằng thống kê mô tả và đồ họa để mô tả và tóm tắt các biến của chúng ta. Trong giai đoạn này, bạn sẽ thấy mình đang phân loại các tính năng và xác định mối tương quan của chúng với biến mục tiêu và với nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1405f7b6-5a26-47db-8608-882104ae2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discrete Variable Correlation by Survival using\n",
    "#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\n",
    "for x in data1_x:\n",
    "    if data1[x].dtype != 'float64' :\n",
    "        print('Transported Correlation by:', x)\n",
    "        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n",
    "        print('-'*10, '\\n')      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19568600-f43a-412f-8eb0-0806f5348698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation heatmap of dataset\n",
    "def correlation_heatmap(df):\n",
    "    _ , ax = plt.subplots(figsize =(14, 12))\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "    \n",
    "    _ = sns.heatmap(\n",
    "        df.corr(), \n",
    "        cmap = colormap,\n",
    "        square=True, \n",
    "        cbar_kws={'shrink':.9 }, \n",
    "        ax=ax,\n",
    "        annot=True, \n",
    "        linewidths=0.1,vmax=1.0, linecolor='white',\n",
    "        annot_kws={'fontsize': 5 }\n",
    "    )\n",
    "    \n",
    "    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "\n",
    "correlation_heatmap(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa6420c-be0b-44e5-8005-dc2695777c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pair plots of entire dataset\n",
    "pp = sns.pairplot(data1, hue = 'Transported', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\n",
    "pp.set(xticklabels=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35a1aab-44ae-4dca-8586-079aac64f088",
   "metadata": {},
   "source": [
    "## Bước 5: Mô hình hóa dữ liệu  \n",
    "\n",
    "**Khoa học dữ liệu** là một lĩnh vực đa ngành kết hợp giữa toán học (thống kê, đại số tuyến tính, v.v.), khoa học máy tính (ngôn ngữ lập trình, hệ thống máy tính, v.v.) và quản lý kinh doanh (giao tiếp, kiến thức chuyên môn, v.v.). Hầu hết các nhà khoa học dữ liệu xuất thân từ một trong ba lĩnh vực này, do đó họ có xu hướng nghiêng về lĩnh vực đó. Tuy nhiên, khoa học dữ liệu giống như một chiếc ghế ba chân, không có chân nào quan trọng hơn chân nào. Vì vậy, bước này sẽ đòi hỏi kiến thức nâng cao về toán học. Nhưng đừng lo, chúng ta chỉ cần một cái nhìn tổng quan, và chúng ta sẽ cùng nhau khám phá. Nhờ khoa học máy tính, nhiều công việc phức tạp đã được tự động hóa. Những bài toán từng yêu cầu bằng cấp cao trong toán học hay thống kê, giờ đây chỉ cần vài dòng code. Cuối cùng, chúng ta cần hiểu biết về kinh doanh để tư duy giải quyết vấn đề. Sau cùng, giống như việc huấn luyện một chú chó dẫn đường, máy học từ chúng ta chứ không phải ngược lại.  \n",
    "\n",
    "### **Giới thiệu về Machine Learning (ML)**  \n",
    "Machine Learning (ML) - Học máy - đúng như tên gọi, là việc dạy máy tính \"cách suy nghĩ\" thay vì \"suy nghĩ về cái gì\". Mặc dù lĩnh vực này và dữ liệu lớn (Big Data) đã xuất hiện từ lâu, nhưng gần đây nó ngày càng phổ biến do rào cản tiếp cận thấp hơn, cho cả doanh nghiệp lẫn cá nhân. Điều này vừa là cơ hội vừa là thách thức. Điểm tích cực là ngày càng có nhiều người tiếp cận được các thuật toán mạnh mẽ để giải quyết nhiều vấn đề thực tế hơn. Tuy nhiên, điều tiêu cực là nhiều người sử dụng các công cụ này mà không thực sự hiểu rõ chúng, dẫn đến những kết luận sai lệch.  \n",
    "\n",
    "Trước đây, mình đã từng ví von rằng nếu bạn yêu cầu ai đó đưa cho bạn một tua vít đầu chữ thập (*Philip screwdriver*), nhưng họ lại đưa cho bạn một tua vít dẹt hoặc tệ hơn là một cái búa, điều đó sẽ phản ánh rõ sự thiếu hiểu biết. Trong khoa học dữ liệu, điều này có thể khiến dự án thất bại hoặc thậm chí đưa ra quyết định sai lầm nghiêm trọng. Vì vậy, thay vì chỉ hướng dẫn bạn **làm thế nào**, mình sẽ giải thích **tại sao bạn làm như vậy**.  \n",
    "\n",
    "### **Các loại Machine Learning**\n",
    "Mục đích của ML là giải quyết các vấn đề của con người. ML có thể được chia thành ba loại chính:  \n",
    "\n",
    "1. **Học có giám sát (*Supervised Learning*):** Cung cấp cho mô hình một tập dữ liệu huấn luyện chứa các đầu vào cùng với nhãn đầu ra đúng.  \n",
    "2. **Học không giám sát (*Unsupervised Learning*):** Dữ liệu huấn luyện không có nhãn, mô hình tự phát hiện ra các mẫu từ dữ liệu.  \n",
    "3. **Học củng cố (*Reinforcement Learning*):** Mô hình không được cung cấp ngay đáp án đúng, mà sẽ nhận được phản hồi sau một chuỗi hành động để dần dần học hỏi.  \n",
    "\n",
    "Trong trường hợp của chúng ta, bài toán yêu cầu dự đoán một hành khách có \"sống sót\" hay không, tức là một bài toán **phân loại có giám sát (*Supervised Classification*)**.  \n",
    "\n",
    "### **Các thuật toán Machine Learning phổ biến**\n",
    "Có rất nhiều thuật toán ML, nhưng chúng có thể được chia thành bốn nhóm chính:  \n",
    "- **Phân loại (*Classification*)**: Dùng khi biến mục tiêu (*target variable*) có giá trị rời rạc (như \"sống sót\" hoặc \"không sống sót\").  \n",
    "- **Hồi quy (*Regression*)**: Dùng khi biến mục tiêu có giá trị liên tục (như dự đoán giá nhà).  \n",
    "- **Phân cụm (*Clustering*)**: Tìm các nhóm dữ liệu tương tự mà không cần nhãn.  \n",
    "- **Giảm chiều dữ liệu (*Dimensionality Reduction*)**: Giảm số lượng đặc trưng để mô hình đơn giản hơn.  \n",
    "\n",
    "Vì bài toán của chúng ta là **phân loại (*Classification*)**, ta có thể chọn một trong các thuật toán dưới đây:  \n",
    "- **Ensemble Methods** (Học tập tổ hợp)  \n",
    "- **Generalized Linear Models (GLM)** (Hồi quy tuyến tính, Logistic Regression)  \n",
    "- **Naive Bayes** (Xác suất Bayes)  \n",
    "- **Nearest Neighbors** (*K-NN*)  \n",
    "- **Support Vector Machines (*SVM*)**  \n",
    "- **Decision Trees** (Cây quyết định)  \n",
    "- **Discriminant Analysis** (Phân tích biệt số)  \n",
    "\n",
    "### **Làm thế nào để chọn thuật toán Machine Learning tốt nhất?**\n",
    "Một câu hỏi phổ biến của người mới học ML là **\"Thuật toán nào là tốt nhất?\"**. Câu trả lời là **không có thuật toán nào tốt nhất trong mọi trường hợp**. Điều này được chứng minh qua **Định lý Không bữa trưa miễn phí (*No Free Lunch Theorem - NFLT*)**, nói rằng **không có thuật toán nào vượt trội trong mọi tình huống**. Do đó, cách tiếp cận tốt nhất là thử nghiệm nhiều thuật toán khác nhau, tinh chỉnh chúng và so sánh kết quả.  \n",
    "\n",
    "Một số nghiên cứu đã so sánh các thuật toán ML, chẳng hạn như:  \n",
    "- **Caruana & Niculescu-Mizil (2006):** So sánh các thuật toán phổ biến.  \n",
    "- **Ogutu et al. (2011):** Ứng dụng trong chọn lọc gen.  \n",
    "- **Fernandez-Delgado et al. (2014):** So sánh 179 bộ phân loại từ 17 nhóm thuật toán.  \n",
    "- **Thoma (2016):** So sánh thuật toán trong *Scikit-learn*.  \n",
    "\n",
    "Ngoài ra, có một quan điểm khác: **\"Dữ liệu nhiều quan trọng hơn thuật toán tốt\"**, nghĩa là một thuật toán đơn giản nhưng có nhiều dữ liệu tốt vẫn có thể cho kết quả vượt trội hơn một thuật toán phức tạp nhưng dữ liệu nghèo nàn.  \n",
    "\n",
    "### **Khởi đầu với Decision Tree, Random Forest, và Boosting**\n",
    "Nếu bạn là người mới, mình khuyên bạn nên bắt đầu với **Cây quyết định (*Decision Tree*), Rừng ngẫu nhiên (*Random Forest*) và Boosting**.  \n",
    "- Chúng dễ hiểu và dễ triển khai.  \n",
    "- Chúng dễ tinh chỉnh hơn so với các thuật toán như *SVM*.  \n",
    "\n",
    "Dưới đây, mình sẽ hướng dẫn cách chạy và so sánh nhiều thuật toán ML khác nhau. Tuy nhiên, phần còn lại của bài này sẽ tập trung vào việc học cách mô hình hóa dữ liệu bằng **Decision Trees và các biến thể của nó**. 🚀\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccee436-b9ba-4b84-8ba3-2ccca0f73cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning Algorithm (MLA) Selection and Initialization\n",
    "MLA = [\n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "\n",
    "    #Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model.RidgeClassifierCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    \n",
    "    #Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "    #SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "    \n",
    "    #Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "    XGBClassifier()    \n",
    "    ]\n",
    "\n",
    "#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "#note: this is an alternative to train_test_split\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "#create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "#create table to compare MLA predictions\n",
    "MLA_predict = data1[Target]\n",
    "\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "\n",
    "    #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "\n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
    "    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n",
    "    \n",
    "\n",
    "    #save MLA predictions - see section 6 for usage\n",
    "    alg.fit(data1[data1_x_bin], data1[Target])\n",
    "    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n",
    "    \n",
    "    row_index+=1\n",
    "    \n",
    "#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "MLA_compare\n",
    "#MLA_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4a5d0-a631-4acf-bb7c-0f8089b264af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\n",
    "sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n",
    "\n",
    "#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\n",
    "plt.title('Machine Learning Algorithm Accuracy Score \\n')\n",
    "plt.xlabel('Accuracy Score (%)')\n",
    "plt.ylabel('Algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c811a97-dda0-4e0e-afa4-2f7fbfaa39fd",
   "metadata": {},
   "source": [
    "## 5.1 Đánh giá Hiệu suất Mô hình  \n",
    "\n",
    "Hãy cùng tổng kết lại: với một số bước làm sạch dữ liệu cơ bản, phân tích dữ liệu và áp dụng thuật toán Machine Learning (MLA), chúng ta có thể dự đoán việc vận chuyển hành khách với **độ chính xác khoảng 80%**. Không tệ chỉ với vài dòng code.  \n",
    "\n",
    "Nhưng câu hỏi luôn được đặt ra là: **Chúng ta có thể làm tốt hơn không?** Và quan trọng hơn, **việc đầu tư thời gian có xứng đáng không?** Ví dụ, nếu chúng ta chỉ tăng độ chính xác thêm **0.1%**, nhưng phải mất **3 tháng phát triển**, thì có thực sự đáng không? Nếu bạn làm trong lĩnh vực nghiên cứu, câu trả lời có thể là \"có\". Nhưng nếu bạn làm trong kinh doanh, phần lớn câu trả lời sẽ là \"không\". Vì vậy, hãy luôn cân nhắc điều này khi cải thiện mô hình của mình.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Khoa học Dữ liệu 101: Xác định Độ chính xác Cơ bản (Baseline Accuracy)**  \n",
    "\n",
    "Trước khi quyết định **cải thiện mô hình**, chúng ta cần xác định xem mô hình hiện tại **có đáng để giữ lại không**. Để làm điều đó, hãy quay về **kiến thức cơ bản của khoa học dữ liệu**.  \n",
    "\n",
    "🟢 **Đây là một bài toán nhị phân**, vì chỉ có hai kết quả có thể xảy ra:  \n",
    "1️⃣ Hành khách được vận chuyển.  \n",
    "2️⃣ Hành khách không được vận chuyển.  \n",
    "\n",
    "Hãy nghĩ về nó như một **vấn đề tung đồng xu**. Nếu bạn có một đồng xu công bằng và đoán mặt **ngửa hoặc sấp**, thì **xác suất đoán đúng là 50%**. Vì vậy, chúng ta đặt **50% là mức hiệu suất mô hình thấp nhất**, bởi vì nếu mô hình của bạn còn kém hơn mức này, thì tôi có thể đơn giản **tung đồng xu** thay vì dùng mô hình của bạn.  \n",
    "\n",
    "🚀 **Nhưng chúng ta có dữ liệu!**  \n",
    "- Trong tập dữ liệu của chúng ta, có **4.378 / 8.693 (~50.4%)** hành khách được vận chuyển.  \n",
    "- Nếu chúng ta **luôn dự đoán rằng 100% hành khách sẽ được vận chuyển**, chúng ta sẽ đúng **50.3%** thời gian.  \n",
    "\n",
    "📌 Vì vậy, hãy đặt **51% là mức hiệu suất mô hình kém**, bởi vì nếu mô hình của bạn kém hơn mức này, tôi có thể **đơn giản dự đoán theo tần suất phổ biến nhất** mà vẫn có kết quả tốt hơn.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Khoa học Dữ liệu 101: Cách Tạo Mô hình của Riêng Bạn**  \n",
    "\n",
    "Mô hình của chúng ta đang dần được cải thiện, nhưng liệu chúng ta có thể **làm tốt hơn không**? Liệu có **dấu hiệu (signal)** nào trong dữ liệu không?  \n",
    "\n",
    "🔍 Để minh họa điều này, chúng ta sẽ xây dựng một **mô hình cây quyết định (Decision Tree)**, vì đây là mô hình **dễ hình dung nhất** và chỉ yêu cầu **các phép tính cộng và nhân đơn giản**.  \n",
    "\n",
    "📌 **Cách xây dựng cây quyết định:**  \n",
    "- Chúng ta cần đặt các câu hỏi để **phân nhóm dữ liệu** sao cho nhóm \"được vận chuyển\" (1) và \"không được vận chuyển\" (0) trở nên đồng nhất.  \n",
    "- Đây là **sự kết hợp giữa khoa học và nghệ thuật**, giống như trò chơi \"21 câu hỏi\".  \n",
    "- Nếu bạn muốn thực hành, hãy tải tập dữ liệu huấn luyện, nhập vào **Excel** và tạo **Pivot Table** để phân tích từng yếu tố sau.  \n",
    "\n",
    "🎯 **Mục tiêu:** Xây dựng một cây quyết định để **tách nhóm \"được vận chuyển\" và \"không được vận chuyển\"**.  \n",
    "\n",
    "🚀 **Câu hỏi phân nhóm (Decision Rules):**  \n",
    "1️⃣ **Trẻ em (0-18 tuổi) có tỷ lệ được vận chuyển cao hơn.**  \n",
    "2️⃣ **Những người được vận chuyển có xu hướng chi tiêu ít hơn.**  \n",
    "3️⃣ **Người từ Trái Đất có ít cơ hội được vận chuyển hơn, trong khi người từ châu Âu thì may mắn hơn.**  \n",
    "4️⃣ **Những người ở trong buồng đông lạnh (cryopod) có khả năng được vận chuyển cao.**  \n",
    "5️⃣ **Vị trí của cabin cũng ảnh hưởng đến khả năng vận chuyển.**  \n",
    "\n",
    "📌 **Kết quả:**  \n",
    "- Chỉ với **một số quy tắc đơn giản**, chúng ta đã đạt được **độ chính xác 70%**.  \n",
    "- Nếu chúng ta xếp hạng mô hình theo thang **Tệ - Kém - Tốt - Khá - Xuất sắc**, thì **70% sẽ được xếp vào mức \"Tốt\"**.  \n",
    "- **Nhưng liệu chúng ta có thể làm tốt hơn mô hình tự tạo này không?** 🤔  \n",
    "\n",
    "---\n",
    "\n",
    "### **📌 Trước khi tiếp tục, hãy viết lại các bước trên thành mã Python!**  \n",
    "\n",
    "Hãy nhớ rằng, quá trình trên **được thực hiện thủ công**, nhưng chúng ta có thể **tự động hóa** bằng các thuật toán MLA.  \n",
    "\n",
    "🔍 **Ví MLA giống như một máy tính TI-89 trong kỳ thi Toán**.  \n",
    "- Nó **rất mạnh mẽ** và giúp bạn thực hiện hầu hết các công việc tính toán.  \n",
    "- **Nhưng nếu bạn không hiểu bài toán**, thì ngay cả máy tính mạnh nhất cũng **không thể giúp bạn giải bài**.  \n",
    "\n",
    "📝 Vì vậy, hãy nghiên cứu kỹ phần tiếp theo trước khi đi sâu vào Machine Learning! 🚀  \n",
    "\n",
    "📖 **Tài liệu tham khảo:**  \n",
    "- [Hướng dẫn về Cross-Validation và Decision Tree](http://www.cs.utoronto.ca/~fidler/teaching/2015/slides/CSC411/tutorial3_CrossVal-DTs.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e6838-2d8b-4c1b-89dd-e43cac919596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT: This is a handmade model for learning purposes only.\n",
    "#However, it is possible to create your own predictive model without a fancy algorithm :)\n",
    "\n",
    "#coin flip model with random 1/survived 0/died\n",
    "\n",
    "#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\n",
    "for index, row in data1.iterrows(): \n",
    "    #random number generator: https://docs.python.org/2/library/random.html\n",
    "    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n",
    "        data1.at[index, 'Random_Predict'] = 1 # predict survived/1\n",
    "    else: \n",
    "        data1.at[index, 'Random_Predict'] = 0 # predict died/0\n",
    "    \n",
    "\n",
    "#score random guess of survival. Use shortcut 1 = Right Guess and 0 = Wrong Guess\n",
    "#the mean of the column will then equal the accuracy\n",
    "data1['Random_Score'] = 0 #assume prediction wrong\n",
    "data1.loc[(data1['Transported'] == data1['Random_Predict']), 'Random_Score'] = 1 #set to 1 for correct prediction\n",
    "print('Coin Flip Model Accuracy: {:.2f}%'.format(data1['Random_Score'].mean()*100))\n",
    "\n",
    "#we can also use scikit's accuracy_score function to save us a few lines of code\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "print('Coin Flip Model Accuracy w/SciKit: {:.2f}%'.format(metrics.accuracy_score(data1[Target], data1['Random_Predict'])*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da841568-2183-4ce5-b9f5-a9b7e05c9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec4ad7c-2ac8-43b6-95eb-25cb3df1a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\n",
    "pivot_age = data1.groupby(['Age_group'])['Transported'].mean()\n",
    "print('Survival Decision Tree age Node: \\n',pivot_age)\n",
    "\n",
    "pivot_vip = data1.groupby(['VIP_Code'])['Transported'].mean()\n",
    "print('\\n Survival Decision Tree vip Node: \\n',pivot_vip)\n",
    "\n",
    "pivot_HomePlanet = data1.groupby(['HomePlanet_Code'])['Transported'].mean()\n",
    "print('\\n Survival Decision Tree HomePlanet Node: \\n',pivot_HomePlanet)\n",
    "\n",
    "pivot_CryoSleep = data1.groupby(['CryoSleep_Code'])['Transported'].mean()\n",
    "print('\\n Survival Decision Tree CryoSleep Node: \\n',pivot_CryoSleep)\n",
    "\n",
    "pivot_Cabin_deck_Code = data1.groupby(['Cabin_deck_Code'])['Transported'].mean()\n",
    "print('\\n Survival Decision Tree Cabin_deck_Code Node: \\n',pivot_Cabin_deck_Code)\n",
    "\n",
    "pivot_Cabin_side_Code = data1.groupby(['Cabin_side_Code'])['Transported'].mean()\n",
    "print('\\n Survival Decision Tree Cabin_side_Code Node: \\n',pivot_Cabin_side_Code)\n",
    "\n",
    "# pivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Transported'].mean()\n",
    "# print('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14a645-fb29-4d3e-b1be-9b9b672e2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\n",
    "def mytree(df):\n",
    "    \n",
    "    #initialize table to store predictions\n",
    "    Model = pd.DataFrame(data = {'Predict':[]})\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        #Question 1: Age_group (55-69%)\n",
    "        if (df.loc[index, 'Age_group'] == 'Age_0-12') or (df.loc[index, 'Age_group'] == 'Age_13-17'):\n",
    "                  Model.loc[index, 'Predict'] = 1\n",
    "                \n",
    "        #Question 2: HomePlanet_Code (66-67%)\n",
    "        if (df.loc[index, 'HomePlanet_Code'] == 0):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "        if (df.loc[index, 'HomePlanet_Code'] == 1):\n",
    "                  Model.loc[index, 'Predict'] = 1 \n",
    "        \n",
    "        #Question 3: VIP_Code (71%)\n",
    "        if (df.loc[index, 'VIP_Code'] == 1):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "        \n",
    "        #Question 4: CryoSleep_Code (68-81%)\n",
    "        if (df.loc[index, 'CryoSleep_Code'] == 0):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "        if (df.loc[index, 'CryoSleep_Code'] == 1):\n",
    "                  Model.loc[index, 'Predict'] = 1  \n",
    "                \n",
    "        #Question 5: Cabin_deck_Code_Code (73-80%)\n",
    "        if (df.loc[index, 'Cabin_deck_Code'] == 7):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "        if (df.loc[index, 'Cabin_deck_Code'] == 1):\n",
    "                  Model.loc[index, 'Predict'] = 1\n",
    "        if (df.loc[index, 'Cabin_deck_Code'] == 2):\n",
    "                  Model.loc[index, 'Predict'] = 1\n",
    "        if (df.loc[index, 'Cabin_deck_Code'] == 4):\n",
    "                  Model.loc[index, 'Predict'] = 0                \n",
    "        \n",
    "        #Question 6: Cabin_side_Code (72%)\n",
    "        if (df.loc[index, 'Cabin_side_Code'] == 2):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "                \n",
    "        \n",
    "    return Model\n",
    "\n",
    "\n",
    "#model data\n",
    "Tree_Predict = mytree(data1)\n",
    "print('Decision Tree Model Accuracy/Precision Score: {:.2f}%\\n'.format(metrics.accuracy_score(data1['Transported'], Tree_Predict)*100))\n",
    "\n",
    "\n",
    "#Accuracy Summary Report with http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report\n",
    "#Where recall score = (true positives)/(true positive + false negative) w/1 being best:http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n",
    "#And F1 score = weighted average of precision and recall w/1 being best: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "print(metrics.classification_report(data1['Transported'], Tree_Predict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495eb74-2f08-4881-b640-3f49ffbea02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Accuracy Summary\n",
    "#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = metrics.confusion_matrix(data1['Transported'], Tree_Predict)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "class_names = ['NotTransported', 'Transported']\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, \n",
    "                      title='Normalized confusion matrix')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209a2c58-11ee-41a5-b4c1-de1baeb32a7d",
   "metadata": {},
   "source": [
    "## 5.11 Hiệu suất Mô hình với Cross-Validation (CV)\n",
    "\n",
    "Ở bước 5.0, chúng ta đã sử dụng hàm [`sklearn cross_validate`](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) để huấn luyện, kiểm tra và đánh giá hiệu suất của mô hình.\n",
    "\n",
    "Hãy nhớ rằng, điều quan trọng là chúng ta phải sử dụng các tập dữ liệu khác nhau cho quá trình huấn luyện và kiểm tra mô hình. Nếu không, mô hình sẽ bị **overfitting** (quá khớp). Điều này có nghĩa là mô hình sẽ hoạt động rất tốt trên dữ liệu đã thấy trước đó, nhưng lại kém hiệu quả khi dự đoán trên dữ liệu mới—và đó không phải là dự đoán thực sự. Nó giống như việc gian lận trong một bài kiểm tra ở trường để đạt điểm tuyệt đối, nhưng khi thi thật thì lại trượt vì thực ra chưa bao giờ thực sự hiểu bài. Điều tương tự cũng xảy ra với machine learning.\n",
    "\n",
    "**Cross-Validation (CV)** là một cách giúp chia tập dữ liệu thành nhiều phần và đánh giá mô hình nhiều lần. Điều này giúp chúng ta có cái nhìn tổng quan hơn về hiệu suất của mô hình trên dữ liệu chưa từng thấy. Dù tốn nhiều tài nguyên tính toán hơn, nhưng đây là một bước quan trọng để tránh có được sự tự tin sai lầm. Điều này đặc biệt hữu ích trong các cuộc thi trên Kaggle hoặc trong bất kỳ trường hợp nào mà tính ổn định và sự chính xác là yếu tố quan trọng.\n",
    "\n",
    "Ngoài CV, chúng ta cũng sử dụng một phiên bản tùy chỉnh của [`sklearn train_test_split`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection), giúp tạo ra sự ngẫu nhiên trong quá trình phân chia dữ liệu kiểm tra. Dưới đây là hình ảnh minh họa về cách chia mặc định của CV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f048437a-e251-4f22-902c-b3c26aa5c71e",
   "metadata": {},
   "source": [
    "<a id=\"ch9\"></a>\n",
    "# 5.12 Điều chỉnh Mô hình với Hyper-Parameters\n",
    "\n",
    "Khi chúng ta sử dụng [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), chúng ta đã chấp nhận tất cả các giá trị mặc định của hàm. Điều này mở ra cơ hội để xem cách các thiết lập khác nhau của **hyper-parameter** sẽ ảnh hưởng đến độ chính xác của mô hình. [(Nhấn vào đây để tìm hiểu thêm về parameters và hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\n",
    "\n",
    "Tuy nhiên, để điều chỉnh một mô hình, trước tiên chúng ta cần thực sự hiểu nó. Đó là lý do tại sao ở các phần trước, tôi đã dành thời gian để giải thích cách dự đoán hoạt động. Bây giờ, hãy cùng tìm hiểu thêm về thuật toán **Decision Tree (DT)**.\n",
    "\n",
    "**Nguồn:** [sklearn](http://scikit-learn.org/stable/modules/tree.html#classification)\n",
    "\n",
    "## ✅ **Ưu điểm của cây quyết định (Decision Tree):**\n",
    "- Dễ hiểu và dễ diễn giải. Cây quyết định có thể được trực quan hóa.\n",
    "- Yêu cầu rất ít công tác chuẩn bị dữ liệu. Trong khi các phương pháp khác thường yêu cầu chuẩn hóa dữ liệu, tạo biến giả (dummy variables), và xử lý giá trị trống thì Decision Tree không cần.\n",
    "- Chi phí tính toán thấp (tỉ lệ logarit với số điểm dữ liệu được sử dụng để huấn luyện cây).\n",
    "- Có thể xử lý cả dữ liệu số và dữ liệu phân loại, trong khi nhiều thuật toán khác chỉ hoạt động tốt với một loại dữ liệu.\n",
    "- Có thể xử lý các bài toán nhiều đầu ra (multi-output).\n",
    "- Là một mô hình **white box** (hộp trắng), dễ dàng giải thích các điều kiện của mô hình bằng logic Boolean. Ngược lại, các mô hình **black box** (hộp đen) như mạng nơ-ron nhân tạo (ANN) có thể rất khó diễn giải.\n",
    "- Có thể kiểm tra độ tin cậy của mô hình bằng các bài kiểm tra thống kê.\n",
    "- Hoạt động tốt ngay cả khi các giả định của mô hình không hoàn toàn chính xác.\n",
    "\n",
    "## ❌ **Nhược điểm của cây quyết định:**\n",
    "- Có thể tạo ra các cây quyết định quá phức tạp, không tổng quát hóa dữ liệu tốt (**overfitting**). Để khắc phục, cần sử dụng các cơ chế như **pruning** (cắt tỉa), đặt giới hạn số mẫu tối thiểu tại một node lá, hoặc đặt độ sâu tối đa của cây.\n",
    "- Nhạy cảm với dữ liệu: Chỉ cần một chút thay đổi trong tập dữ liệu, cây quyết định có thể thay đổi hoàn toàn.\n",
    "- Bài toán tìm cây quyết định tối ưu là **NP-complete**, do đó hầu hết các thuật toán hiện nay đều dựa trên các phương pháp tham lam (**greedy algorithm**) để tìm lời giải cục bộ.\n",
    "- Một số loại bài toán khó học bằng cây quyết định, ví dụ như bài toán **XOR**, **parity**, hoặc **multiplexer**.\n",
    "- Có thể tạo ra các cây có độ chệch cao nếu một số lớp dữ liệu chiếm ưu thế. Do đó, nên cân bằng tập dữ liệu trước khi huấn luyện mô hình.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436796cd-6e36-4dd5-9c2c-b037acd67e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model\n",
    "dtree = tree.DecisionTreeClassifier(random_state = 0)\n",
    "base_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "dtree.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print('BEFORE DT Parameters: ', dtree.get_params())\n",
    "print(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "#print(\"BEFORE DT Test w/bin set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune hyper-parameters: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "param_grid = {'criterion': ['gini', 'entropy'],  #scoring methodology; two supported formulas for calculating information gain - default is gini\n",
    "              #'splitter': ['best', 'random'], #splitting methodology; two supported strategies - default is best\n",
    "              'max_depth': [2,4,6,8,10,None], #max depth tree can grow; default is none\n",
    "              #'min_samples_split': [2,5,10,.03,.05], #minimum subset size BEFORE new split (fraction is % of total); default is 2\n",
    "              #'min_samples_leaf': [1,5,10,.03,.05], #minimum subset size AFTER new split split (fraction is % of total); default is 1\n",
    "              #'max_features': [None, 'auto'], #max features to consider when performing split; default none or all\n",
    "              'random_state': [0] #seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n",
    "             }\n",
    "\n",
    "#print(list(model_selection.ParameterGrid(param_grid)))\n",
    "\n",
    "#choose best model with grid_search: #http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n",
    "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
    "tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True )\n",
    "tune_model.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "#print(tune_model.cv_results_.keys())\n",
    "#print(tune_model.cv_results_['params'])\n",
    "print('AFTER DT Parameters: ', tune_model.best_params_)\n",
    "#print(tune_model.cv_results_['mean_train_score'])\n",
    "print(\"AFTER DT Training w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(tune_model.cv_results_['mean_test_score'])\n",
    "print(\"AFTER DT Test w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT Test w/bin score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#duplicates gridsearchcv\n",
    "#tune_results = model_selection.cross_validate(tune_model, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
    "\n",
    "#print('AFTER DT Parameters: ', tune_model.best_params_)\n",
    "#print(\"AFTER DT Training w/bin set score mean: {:.2f}\". format(tune_results['train_score'].mean()*100)) \n",
    "#print(\"AFTER DT Test w/bin set score mean: {:.2f}\". format(tune_results['test_score'].mean()*100))\n",
    "#print(\"AFTER DT Test w/bin set score min: {:.2f}\". format(tune_results['test_score'].min()*100))\n",
    "#print('-'*10)\n",
    "#base model\n",
    "dtree = tree.DecisionTreeClassifier(random_state = 0)\n",
    "base_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "dtree.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print('BEFORE DT Parameters: ', dtree.get_params())\n",
    "print(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "#print(\"BEFORE DT Test w/bin set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune hyper-parameters: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "param_grid = {'criterion': ['gini', 'entropy'],  #scoring methodology; two supported formulas for calculating information gain - default is gini\n",
    "              #'splitter': ['best', 'random'], #splitting methodology; two supported strategies - default is best\n",
    "              'max_depth': [2,4,6,8,10,None], #max depth tree can grow; default is none\n",
    "              #'min_samples_split': [2,5,10,.03,.05], #minimum subset size BEFORE new split (fraction is % of total); default is 2\n",
    "              #'min_samples_leaf': [1,5,10,.03,.05], #minimum subset size AFTER new split split (fraction is % of total); default is 1\n",
    "              #'max_features': [None, 'auto'], #max features to consider when performing split; default none or all\n",
    "              'random_state': [0] #seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n",
    "             }\n",
    "\n",
    "#print(list(model_selection.ParameterGrid(param_grid)))\n",
    "\n",
    "#choose best model with grid_search: #http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n",
    "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
    "tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True )\n",
    "tune_model.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "#print(tune_model.cv_results_.keys())\n",
    "#print(tune_model.cv_results_['params'])\n",
    "print('AFTER DT Parameters: ', tune_model.best_params_)\n",
    "#print(tune_model.cv_results_['mean_train_score'])\n",
    "print(\"AFTER DT Training w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(tune_model.cv_results_['mean_test_score'])\n",
    "print(\"AFTER DT Test w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT Test w/bin score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#duplicates gridsearchcv\n",
    "#tune_results = model_selection.cross_validate(tune_model, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
    "\n",
    "#print('AFTER DT Parameters: ', tune_model.best_params_)\n",
    "#print(\"AFTER DT Training w/bin set score mean: {:.2f}\". format(tune_results['train_score'].mean()*100)) \n",
    "#print(\"AFTER DT Test w/bin set score mean: {:.2f}\". format(tune_results['test_score'].mean()*100))\n",
    "#print(\"AFTER DT Test w/bin set score min: {:.2f}\". format(tune_results['test_score'].min()*100))\n",
    "#print('-'*10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164b68d9-8510-486f-aba2-c734ae9cead1",
   "metadata": {},
   "source": [
    "<a id=\"ch10\"></a>  \n",
    "## 5.13 Tinh chỉnh mô hình với lựa chọn đặc trưng  \n",
    "\n",
    "Như đã đề cập từ đầu, có nhiều biến dự đoán hơn không có nghĩa là mô hình sẽ tốt hơn, mà quan trọng là chọn đúng biến dự đoán. Vì vậy, một bước quan trọng trong quá trình xây dựng mô hình là **lựa chọn đặc trưng**.  \n",
    "\n",
    "[Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) cung cấp nhiều phương pháp để thực hiện lựa chọn đặc trưng. Trong phần này, chúng ta sẽ sử dụng **loại bỏ đặc trưng đệ quy (Recursive Feature Elimination - RFE) kết hợp với Cross Validation (CV)** để tối ưu hóa việc lựa chọn đặc trưng.  \n",
    "\n",
    "Tham khảo tài liệu về RFE với CV tại đây: [RFECV - Sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd40809-84f9-4f39-94db-d758207762da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model\n",
    "print('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \n",
    "print('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n",
    "\n",
    "print(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "\n",
    "#feature selection\n",
    "dtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split)\n",
    "dtree_rfe.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "#transform x&y to reduced features and fit new model\n",
    "#alternative: can use pipeline to reduce fit and transform steps: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "X_rfe = data1[data1_x_bin].columns.values[dtree_rfe.get_support()]\n",
    "rfe_results = model_selection.cross_validate(dtree, data1[X_rfe], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "\n",
    "#print(dtree_rfe.grid_scores_)\n",
    "print('AFTER DT RFE Training Shape New: ', data1[X_rfe].shape) \n",
    "print('AFTER DT RFE Training Columns New: ', X_rfe)\n",
    "\n",
    "print(\"AFTER DT RFE Training w/bin score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \n",
    "print(\"AFTER DT RFE Test w/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\n",
    "print(\"AFTER DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune rfe model\n",
    "rfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True )\n",
    "rfe_tune_model.fit(data1[X_rfe], data1[Target])\n",
    "\n",
    "#print(rfe_tune_model.cv_results_.keys())\n",
    "#print(rfe_tune_model.cv_results_['params'])\n",
    "print('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)\n",
    "#print(rfe_tune_model.cv_results_['mean_train_score'])\n",
    "print(\"AFTER DT RFE Tuned Training w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(rfe_tune_model.cv_results_['mean_test_score'])\n",
    "print(\"AFTER DT RFE Tuned Test w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT RFE Tuned Test w/bin score 3*std: +/- {:.2f}\". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79f49b-a5c0-4154-8f14-aa23f54dcf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
    "import graphviz \n",
    "dot_data = tree.export_graphviz(dtree, out_file=None, \n",
    "                                feature_names = data1_x_bin, class_names = True,\n",
    "                                filled = True, rounded = True)\n",
    "graph = graphviz.Source(dot_data) \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873e2bd5-0951-4c88-a5af-12381853c5a6",
   "metadata": {},
   "source": [
    "<a id=\"ch11\"></a>\n",
    "# Bước 6: Xác thực và Triển khai\n",
    "\n",
    "Bước tiếp theo là chuẩn bị cho việc nộp kết quả bằng cách sử dụng dữ liệu kiểm tra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c1bcd7-4906-461e-99af-bc5647cc207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n",
    "#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\n",
    "correlation_heatmap(MLA_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d44386a-13e8-4b6a-8419-a0bbca9b82c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#why choose one model, when you can pick them all with voting classifier\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\n",
    "\n",
    "vote_est = [\n",
    "    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n",
    "    ('ada', ensemble.AdaBoostClassifier()),\n",
    "    ('bc', ensemble.BaggingClassifier()),\n",
    "    ('etc',ensemble.ExtraTreesClassifier()),\n",
    "    ('gbc', ensemble.GradientBoostingClassifier()),\n",
    "    ('rfc', ensemble.RandomForestClassifier()),\n",
    "\n",
    "    #Gaussian Processes: http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc\n",
    "    ('gpc', gaussian_process.GaussianProcessClassifier()),\n",
    "    \n",
    "    #GLM: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "    ('lr', linear_model.LogisticRegressionCV()),\n",
    "    \n",
    "    #Navies Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "    ('bnb', naive_bayes.BernoulliNB()),\n",
    "    ('gnb', naive_bayes.GaussianNB()),\n",
    "    \n",
    "    #Nearest Neighbor: http://scikit-learn.org/stable/modules/neighbors.html\n",
    "    ('knn', neighbors.KNeighborsClassifier()),\n",
    "    \n",
    "    #SVM: http://scikit-learn.org/stable/modules/svm.html\n",
    "    ('svc', svm.SVC(probability=True)),\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "   ('xgb', XGBClassifier())\n",
    "\n",
    "]\n",
    "\n",
    "#Hard Vote or majority rules\n",
    "vote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
    "vote_hard_cv = model_selection.cross_validate(vote_hard, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "vote_hard.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Hard Voting Training w/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \n",
    "print(\"Hard Voting Test w/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\n",
    "print(\"Hard Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "#Soft Vote or weighted probabilities\n",
    "vote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n",
    "vote_soft_cv = model_selection.cross_validate(vote_soft, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "vote_soft.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Soft Voting Training w/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \n",
    "print(\"Soft Voting Test w/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\n",
    "print(\"Soft Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f7660-b839-4dbd-bdfe-97795db245e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING: Running is very computational intensive and time expensive.\n",
    "#Code is written for experimental/developmental purposes and not production ready!\n",
    "\n",
    "#Hyperparameter Tune with GridSearchCV: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "grid_n_estimator = [10, 50, 100, 300]\n",
    "grid_ratio = [.1, .25, .5, .75, 1.0]\n",
    "grid_learn = [.01, .03, .05, .1, .25]\n",
    "grid_max_depth = [2, 4, 6, 8, 10, None]\n",
    "grid_min_samples = [5, 10, .03, .05, .10]\n",
    "grid_criterion = ['gini', 'entropy']\n",
    "grid_bool = [True, False]\n",
    "grid_seed = [0]\n",
    "\n",
    "\n",
    "grid_param = [\n",
    "            [{\n",
    "            #AdaBoostClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "            'n_estimators': grid_n_estimator, #default=50\n",
    "            'learning_rate': grid_learn, #default=1\n",
    "            #'algorithm': ['SAMME', 'SAMME.R'], #default=’SAMME.R\n",
    "            'random_state': grid_seed\n",
    "            }],       \n",
    "    \n",
    "            [{\n",
    "            #BaggingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'max_samples': grid_ratio, #default=1.0\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{\n",
    "            #ExtraTreesClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'criterion': grid_criterion, #default=”gini”\n",
    "            'max_depth': grid_max_depth, #default=None\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "            [{\n",
    "            #GradientBoostingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n",
    "            #'loss': ['deviance', 'exponential'], #default=’deviance’\n",
    "            'learning_rate': [.05], #default=0.1 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n",
    "            'n_estimators': [300], #default=100 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n",
    "            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=”friedman_mse”\n",
    "            'max_depth': grid_max_depth, #default=3   \n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{\n",
    "            #RandomForestClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'criterion': grid_criterion, #default=”gini”\n",
    "            'max_depth': grid_max_depth, #default=None\n",
    "            'oob_score': [True], #default=False -- 12/31/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{    \n",
    "            #GaussianProcessClassifier\n",
    "            'max_iter_predict': grid_n_estimator, #default: 100\n",
    "            'random_state': grid_seed\n",
    "            }],        \n",
    "    \n",
    "            [{\n",
    "            #LogisticRegressionCV - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n",
    "            'fit_intercept': grid_bool, #default: True\n",
    "            #'penalty': ['l1','l2'],\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n",
    "            'random_state': grid_seed\n",
    "             }],            \n",
    "    \n",
    "            [{\n",
    "            #BernoulliNB - http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n",
    "            'alpha': grid_ratio, #default: 1.0\n",
    "             }],    \n",
    "    \n",
    "            #GaussianNB - \n",
    "            [{}],\n",
    "    \n",
    "            [{\n",
    "            #KNeighborsClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "            'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n",
    "            'weights': ['uniform', 'distance'], #default = ‘uniform’\n",
    "            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "            }],            \n",
    "    \n",
    "            [{\n",
    "            #SVC - http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "            #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r\n",
    "            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'C': [1,2,3,4,5], #default=1.0\n",
    "            'gamma': grid_ratio, #edfault: auto\n",
    "            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n",
    "            'probability': [True],\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{\n",
    "            #XGBClassifier - http://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "            'learning_rate': grid_learn, #default: .3\n",
    "            'max_depth': [1,2,4,6,8,10], #default 2\n",
    "            'n_estimators': grid_n_estimator, \n",
    "            'seed': grid_seed  \n",
    "             }]   \n",
    "        ]\n",
    "\n",
    "start_total = time.perf_counter() #https://docs.python.org/3/library/time.html#time.perf_counter\n",
    "for clf, param in zip (vote_est, grid_param): #https://docs.python.org/3/library/functions.html#zip\n",
    "\n",
    "    #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm\n",
    "    #print(param)    \n",
    "    \n",
    "    start = time.perf_counter()        \n",
    "    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n",
    "    best_search.fit(data1[data1_x_bin], data1[Target])\n",
    "    run = time.perf_counter() - start\n",
    "\n",
    "    best_param = best_search.best_params_\n",
    "    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n",
    "    clf[1].set_params(**best_param) \n",
    "\n",
    "\n",
    "run_total = time.perf_counter() - start_total\n",
    "print('Total optimization time was {:.2f} minutes.'.format(run_total/60))\n",
    "\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54263ddc-f0cf-4bb7-ba3f-afb166aea58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hard Vote or majority rules w/Tuned Hyperparameters\n",
    "grid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
    "grid_hard_cv = model_selection.cross_validate(grid_hard, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True)\n",
    "grid_hard.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Hard Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \n",
    "print(\"Hard Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\n",
    "print(\"Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "#Soft Vote or weighted probabilities w/Tuned Hyperparameters\n",
    "grid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n",
    "grid_soft_cv = model_selection.cross_validate(grid_soft, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True)\n",
    "grid_soft.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Soft Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \n",
    "print(\"Soft Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\n",
    "print(\"Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d06211-537c-473a-ae7c-4680c5c8dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for modeling\n",
    "print(data_val.info())\n",
    "print(\"-\"*10)\n",
    "#data_val.sample(10)\n",
    "\n",
    "#handmade decision tree - submission score = 0.77990\n",
    "# data_val['Transported'] = mytree(data_val).astype(int)  # 0 V7\n",
    "data_val['Transported'] = mytree(data_val)\n",
    "\n",
    "#decision tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n",
    "#submit_dt = tree.DecisionTreeClassifier()\n",
    "#submit_dt = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_dt.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_dt.best_params_) #Best Parameters:  {'criterion': 'gini', 'max_depth': 4, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_dt.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\n",
    "#submit_bc = ensemble.BaggingClassifier()\n",
    "#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_bc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n",
    "#submit_etc = ensemble.ExtraTreesClassifier()\n",
    "#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_etc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\n",
    "#submit_rfc = ensemble.RandomForestClassifier()\n",
    "#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_rfc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\n",
    "#submit_abc = ensemble.AdaBoostClassifier()\n",
    "#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_abc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\n",
    "#submit_gbc = ensemble.GradientBoostingClassifier()\n",
    "#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_gbc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\n",
    "\n",
    "#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\n",
    "#submit_xgb = XGBClassifier()\n",
    "#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_xgb.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\n",
    "#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#hard voting classifier w/full dataset modeling submission score: defaults=-, tuned = 0.74655 V4\n",
    "# data_val['Transported'] = vote_hard.predict(data_val[data1_x_bin])  # 0.74655 V4\n",
    "# data_val['Transported'] = grid_hard.predict(data_val[data1_x_bin])  # 0.70189 V4\n",
    "\n",
    "\n",
    "#soft voting classifier w/full dataset modeling submission score: defaults=-, tuned = 0.75005 V6\n",
    "# data_val['Transported'] = vote_soft.predict(data_val[data1_x_bin])  # 0.75005 V6\n",
    "# data_val['Transported'] = grid_soft.predict(data_val[data1_x_bin])  # 0.74982 V5\n",
    "\n",
    "\n",
    "#submit file\n",
    "submit = data_val[['PassengerId','Transported']]\n",
    "submit.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print('Validation Data Distribution: \\n', data_val['Transported'].value_counts(normalize = True))\n",
    "submit.sample(10)\n",
    "\n",
    "# The best parameter for AdaBoostClassifier is {'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0} with a runtime of 154.92 seconds.\n",
    "# The best parameter for BaggingClassifier is {'max_samples': 0.1, 'n_estimators': 300, 'random_state': 0} with a runtime of 269.41 seconds.\n",
    "# The best parameter for ExtraTreesClassifier is {'criterion': 'gini', 'max_depth': 8, 'n_estimators': 300, 'random_state': 0} with a runtime of 284.94 seconds.\n",
    "# The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 300, 'random_state': 0} with a runtime of 497.71 seconds.\n",
    "# The best parameter for RandomForestClassifier is {'criterion': 'gini', 'max_depth': 8, 'n_estimators': 300, 'oob_score': True, 'random_state': 0} with a runtime of 396.49 seconds.\n",
    "# The best parameter for GaussianProcessClassifier is {'max_iter_predict': 10, 'random_state': 0} with a runtime of 1470.09 seconds.\n",
    "# The best parameter for LogisticRegressionCV is {'fit_intercept': True, 'random_state': 0, 'solver': 'lbfgs'} with a runtime of 445.46 seconds.\n",
    "# The best parameter for BernoulliNB is {'alpha': 0.5} with a runtime of 1.12 seconds.\n",
    "# The best parameter for GaussianNB is {} with a runtime of 0.26 seconds.\n",
    "# The best parameter for KNeighborsClassifier is {'algorithm': 'ball_tree', 'n_neighbors': 7, 'weights': 'distance'} with a runtime of 47.07 seconds.\n",
    "# The best parameter for SVC is {'C': 1, 'decision_function_shape': 'ovo', 'gamma': 0.1, 'probability': True, 'random_state': 0} with a runtime of 5298.85 seconds.\n",
    "# The best parameter for XGBClassifier is {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 50, 'seed': 0} with a runtime of 614.08 seconds.\n",
    "# Total optimization time was 158.01 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d5efb-6d7b-4e10-9502-b7c44f717d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e5de6-fcec-4f50-a58e-1a1d64d5ab1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5efe86c-9e8f-4cd7-b4fb-0994a3d7effe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
